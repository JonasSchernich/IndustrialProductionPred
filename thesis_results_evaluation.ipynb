{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63d49485",
   "metadata": {},
   "source": [
    "# Thesis Results Notebook (Real Outputs)\n",
    "Dieses Notebook lädt **Stage‑B Outputs** aus `outputs/` und erstellt die Tabellen/Grafiken für deine Results-Sektion.\n",
    "\n",
    "**Du musst nur** die Run‑Ordnernamen und Labels in den Mappings anpassen.\n",
    "\n",
    "Erwartete Dateien pro Run:\n",
    "- `outputs/stageB/<run_id>/monthly/preds.csv` (mit Spalten: `date_t_plus_1`, `y_true`, `y_pred`, `is_active`)\n",
    "- optional: `outputs/stageB/<run_id>/monthly/scores.csv` (für Incumbent Timeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "9128aa1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T08:51:59.758906Z",
     "start_time": "2026-01-20T08:51:56.059344Z"
    }
   },
   "source": [
    "\n",
    "# === 0) Imports ===\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# Optional (für MCS)\n",
    "from arch.bootstrap import MCS\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option(\"display.max_columns\", 200)\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d606565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 1) Projekt-Root finden & src importieren ===\n",
    "def _locate_repo_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    for _ in range(8):\n",
    "        if (cur / \"src\").exists():\n",
    "            return cur\n",
    "        if cur.parent == cur:\n",
    "            break\n",
    "        cur = cur.parent\n",
    "    return start.resolve()\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = _locate_repo_root(NOTEBOOK_DIR)\n",
    "os.environ[\"PROJECT_ROOT\"] = str(PROJECT_ROOT)\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.config import STAGEB_DIR, OUTPUTS\n",
    "\n",
    "EVAL_OUT = OUTPUTS / \"evaluation_results\"\n",
    "FIG_OUT = EVAL_OUT / \"figures\"\n",
    "TAB_OUT = EVAL_OUT / \"tables\"\n",
    "FIG_OUT.mkdir(parents=True, exist_ok=True)\n",
    "TAB_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "print(\"StageB dir:   \", STAGEB_DIR)\n",
    "print(\"Eval out:     \", EVAL_OUT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53030ca",
   "metadata": {},
   "source": [
    "## 2) Konfiguration: Run‑Mappings\n",
    "- `run_id`: Ordnername in `outputs/stageB/<run_id>/...`\n",
    "- `label`: Anzeige‑Name (Plots/Tabellen)\n",
    "- `family`: Modellfamilie (für Aggregationen/Plots)\n",
    "- `setup`: `Setup I`, `Setup II`, `Setup III` (oder `Baseline`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c183eca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 2) EDIT THIS: Model Registry ===\n",
    "MODEL_REGISTRY = [\n",
    "    # Setup I (ifo only)\n",
    "    dict(run_id=\"Model1_without_target\", label=\"Model 1 (ifo only)\", family=\"FamilyA\", setup=\"Setup I\"),\n",
    "    dict(run_id=\"Model2_without_target\", label=\"Model 2 (ifo only)\", family=\"FamilyB\", setup=\"Setup I\"),\n",
    "\n",
    "    # Setup II (ifo + target blocks)\n",
    "    dict(run_id=\"Model1_with_target\", label=\"Model 1 (+target blocks)\", family=\"FamilyA\", setup=\"Setup II\"),\n",
    "    dict(run_id=\"Model2_with_target\", label=\"Model 2 (+target blocks)\", family=\"FamilyB\", setup=\"Setup II\"),\n",
    "\n",
    "    # Setup III (few features / FI selection)\n",
    "    dict(run_id=\"Model1_fi\", label=\"Model 1 (FI top)\", family=\"FamilyA\", setup=\"Setup III\"),\n",
    "    dict(run_id=\"Model2_fi\", label=\"Model 2 (FI top)\", family=\"FamilyB\", setup=\"Setup III\"),\n",
    "]\n",
    "\n",
    "# === 2b) EDIT THIS: Baselines (optional) ===\n",
    "BASELINE_REGISTRY = [\n",
    "    dict(run_id=\"Baseline_random_walk\", label=\"Random Walk\", family=\"Baseline\", setup=\"Baseline\"),\n",
    "    dict(run_id=\"Baseline_ar1\", label=\"AR(1)\", family=\"Baseline\", setup=\"Baseline\"),\n",
    "    dict(run_id=\"Baseline_expmean\", label=\"Exp. Mean\", family=\"Baseline\", setup=\"Baseline\"),\n",
    "]\n",
    "\n",
    "SETUP_ORDER = [\"Setup I\", \"Setup II\", \"Setup III\"]\n",
    "\n",
    "# Signifikanz/Tests\n",
    "ALPHA_SIG = 0.05\n",
    "HAC_LAGS = 3\n",
    "\n",
    "# MCS\n",
    "MCS_ALPHA = 0.10\n",
    "MCS_BLOCK_LEN = 6\n",
    "\n",
    "# Rolling plot\n",
    "ROLLING_WINDOW_MONTHS = 24\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef91ab1",
   "metadata": {},
   "source": [
    "## 3) Loader: aktive Stage‑B Prognosen (`is_active=True`)\n",
    "Wir verwenden die **aktive Policy‑Sequenz** (wie in deinen Test‑Notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d09efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _stageb_preds_path(run_id: str) -> Path:\n",
    "    return STAGEB_DIR / run_id / \"monthly\" / \"preds.csv\"\n",
    "\n",
    "def load_active_stageb_predictions(run_id: str) -> pd.DataFrame:\n",
    "    \"\"\"Return DataFrame indexed by date_t_plus_1 with columns y_true, y_pred (active rows only).\"\"\"\n",
    "    path = _stageb_preds_path(run_id)\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing: {path}\")\n",
    "\n",
    "    df = pd.read_csv(path, parse_dates=[\"date_t_plus_1\"])\n",
    "    df = df[df[\"is_active\"] == True].copy()\n",
    "    if df.empty:\n",
    "        raise ValueError(f\"No is_active==True rows found in: {path}\")\n",
    "\n",
    "    df = df.sort_values(\"date_t_plus_1\").set_index(\"date_t_plus_1\")\n",
    "    df = df[[\"y_true\", \"y_pred\"]].copy()\n",
    "    df = df[~df.index.duplicated(keep=\"last\")]\n",
    "    return df\n",
    "\n",
    "def load_many_active_predictions(registry):\n",
    "    out = {}\n",
    "    for row in registry:\n",
    "        rid = row[\"run_id\"]\n",
    "        try:\n",
    "            out[rid] = load_active_stageb_predictions(rid)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] {rid}: {e}\")\n",
    "    return out\n",
    "\n",
    "stageb_active = load_many_active_predictions(MODEL_REGISTRY + BASELINE_REGISTRY)\n",
    "print(f\"Loaded active Stage-B runs: {len(stageb_active)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c279054c",
   "metadata": {},
   "source": [
    "## 4) Metrics & Tests (RMSE/MAE, DM, MCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7712c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rmse(y_true, y_pred) -> float:\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
    "\n",
    "def mae(y_true, y_pred) -> float:\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    return float(np.mean(np.abs(y_true - y_pred)))\n",
    "\n",
    "def losses_se(df_pred: pd.DataFrame) -> pd.Series:\n",
    "    return (df_pred[\"y_true\"] - df_pred[\"y_pred\"]) ** 2\n",
    "\n",
    "def dm_test_hac(loss_a: pd.Series, loss_b: pd.Series, hac_lags: int = HAC_LAGS, alternative: str = \"two-sided\"):\n",
    "    \"\"\"DM as HAC-robust regression of d_t = loss_a - loss_b on constant.\"\"\"\n",
    "    df = pd.concat([loss_a.rename(\"a\"), loss_b.rename(\"b\")], axis=1).dropna()\n",
    "    d = df[\"a\"] - df[\"b\"]\n",
    "\n",
    "    X = np.ones((len(d), 1))\n",
    "    fit = sm.OLS(d.values, X).fit(cov_type=\"HAC\", cov_kwds={\"maxlags\": hac_lags})\n",
    "    t = float(fit.tvalues[0])\n",
    "    p2 = float(fit.pvalues[0])  # two-sided from statsmodels\n",
    "\n",
    "    if alternative == \"two-sided\":\n",
    "        p = p2\n",
    "    elif alternative == \"less\":\n",
    "        # H1: mean(d) < 0  (A better than B)\n",
    "        p = p2 / 2 if t < 0 else 1 - (p2 / 2)\n",
    "    elif alternative == \"greater\":\n",
    "        # H1: mean(d) > 0  (A worse than B)\n",
    "        p = p2 / 2 if t > 0 else 1 - (p2 / 2)\n",
    "    else:\n",
    "        raise ValueError(\"alternative must be 'two-sided', 'less', or 'greater'\")\n",
    "\n",
    "    return t, float(p), int(len(d))\n",
    "\n",
    "def compute_overall_metrics(df_pred: pd.DataFrame) -> dict:\n",
    "    y_true = df_pred[\"y_true\"].to_numpy()\n",
    "    y_pred = df_pred[\"y_pred\"].to_numpy()\n",
    "    return {\n",
    "        \"rmse\": rmse(y_true, y_pred),\n",
    "        \"mae\": mae(y_true, y_pred),\n",
    "        \"n\": int(len(df_pred)),\n",
    "        \"start\": df_pred.index.min(),\n",
    "        \"end\": df_pred.index.max(),\n",
    "    }\n",
    "\n",
    "def align_losses_matrix(run_ids):\n",
    "    series = []\n",
    "    for rid in run_ids:\n",
    "        dfp = stageb_active[rid]\n",
    "        series.append(losses_se(dfp).rename(rid))\n",
    "    return pd.concat(series, axis=1).dropna()\n",
    "\n",
    "def compute_mcs_inclusion(loss_mat: pd.DataFrame, alpha: float = MCS_ALPHA, block_size: int = MCS_BLOCK_LEN):\n",
    "    proc = MCS(loss_mat, size=alpha, block_size=block_size, method=\"stationary\")\n",
    "    proc.compute()\n",
    "    return set(proc.included), set(proc.excluded), proc\n",
    "\n",
    "def fmt_p(p):\n",
    "    if pd.isna(p):\n",
    "        return \"\"\n",
    "    if p < 0.01:\n",
    "        return \"<0.01\"\n",
    "    if p > 0.99:\n",
    "        return \"1.00\"\n",
    "    return f\"{p:.2f}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67a6293",
   "metadata": {},
   "source": [
    "## 5) Table R1: pro Setup – RMSE, MAE, DM vs best competitor, MCS badge\n",
    "**DM ist hier pairwise** (jeder gegen den besten im Setup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199c4c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_table_r1_for_setup(setup: str) -> pd.DataFrame:\n",
    "    rows = [r for r in MODEL_REGISTRY if r[\"setup\"] == setup and r[\"run_id\"] in stageb_active]\n",
    "    if len(rows) == 0:\n",
    "        raise ValueError(f\"No runs loaded for setup={setup}\")\n",
    "\n",
    "    metrics = []\n",
    "    for r in rows:\n",
    "        m = compute_overall_metrics(stageb_active[r[\"run_id\"]])\n",
    "        metrics.append(dict(\n",
    "            run_id=r[\"run_id\"], label=r[\"label\"], family=r[\"family\"], setup=r[\"setup\"],\n",
    "            rmse=m[\"rmse\"], mae=m[\"mae\"], n=m[\"n\"],\n",
    "        ))\n",
    "    dfm = pd.DataFrame(metrics).sort_values(\"rmse\")\n",
    "    best_run = dfm.iloc[0][\"run_id\"]\n",
    "\n",
    "    dm_p = {}\n",
    "    for rid in dfm[\"run_id\"]:\n",
    "        if rid == best_run:\n",
    "            dm_p[rid] = np.nan\n",
    "            continue\n",
    "        _, p, _ = dm_test_hac(\n",
    "            losses_se(stageb_active[rid]),\n",
    "            losses_se(stageb_active[best_run]),\n",
    "            alternative=\"two-sided\"\n",
    "        )\n",
    "        dm_p[rid] = p\n",
    "    dfm[\"dm_p_vs_best\"] = dfm[\"run_id\"].map(dm_p)\n",
    "\n",
    "    run_ids = dfm[\"run_id\"].tolist()\n",
    "    loss_mat = align_losses_matrix(run_ids)\n",
    "    included, _, _ = compute_mcs_inclusion(loss_mat)\n",
    "    dfm[\"mcs_in\"] = dfm[\"run_id\"].apply(lambda x: x in included)\n",
    "\n",
    "    dfm = dfm[[\"label\", \"family\", \"rmse\", \"mae\", \"dm_p_vs_best\", \"mcs_in\", \"n\"]].copy()\n",
    "    dfm = dfm.rename(columns={\n",
    "        \"label\": \"Model\",\n",
    "        \"family\": \"Family\",\n",
    "        \"rmse\": \"RMSE\",\n",
    "        \"mae\": \"MAE\",\n",
    "        \"dm_p_vs_best\": \"DM p (vs best)\",\n",
    "        \"mcs_in\": \"MCS\",\n",
    "        \"n\": \"N\",\n",
    "    })\n",
    "    return dfm\n",
    "\n",
    "tables_r1 = {}\n",
    "for s in SETUP_ORDER:\n",
    "    try:\n",
    "        tables_r1[s] = build_table_r1_for_setup(s)\n",
    "        print(f\"[OK] Table R1 for {s}: {tables_r1[s].shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[SKIP] {s}: {e}\")\n",
    "\n",
    "tables_r1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608de675",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Display + export Table R1\n",
    "for setup, df in tables_r1.items():\n",
    "    out_csv = TAB_OUT / f\"Table_R1_{setup.replace(' ', '_')}.csv\"\n",
    "    df.to_csv(out_csv, index=False)\n",
    "\n",
    "    print(\"\\n\", \"=\"*80)\n",
    "    print(f\"Table R1 – {setup}\")\n",
    "    display(df.style.format({\"RMSE\":\"{:.3f}\", \"MAE\":\"{:.3f}\", \"DM p (vs best)\": fmt_p}))\n",
    "    print(\"Saved:\", out_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67336bc",
   "metadata": {},
   "source": [
    "## 6) Figure R1 (pro Setup): Lollipop – RMSE pro Modell (MCS markiert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49472f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_mcs_lollipop_for_setup(setup: str, filename: str | None = None):\n",
    "    df = tables_r1[setup].copy().sort_values(\"RMSE\", ascending=False).reset_index(drop=True)\n",
    "    y = np.arange(len(df))\n",
    "\n",
    "    xmin = df[\"RMSE\"].min() - 0.02\n",
    "    xmax = df[\"RMSE\"].max() + 0.05\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 0.5*len(df) + 2), dpi=120)\n",
    "    ax.hlines(y=y, xmin=xmin, xmax=df[\"RMSE\"], alpha=0.35, linewidth=1)\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        in_mcs = bool(row[\"MCS\"])\n",
    "        marker = \"D\" if in_mcs else \"o\"\n",
    "        face = \"black\" if in_mcs else \"white\"\n",
    "        ax.plot(row[\"RMSE\"], i, marker=marker, markersize=9,\n",
    "                markerfacecolor=face, markeredgecolor=\"black\", markeredgewidth=1.5, linestyle=\"\")\n",
    "        p = row[\"DM p (vs best)\"]\n",
    "        if pd.notna(p):\n",
    "            ax.text(row[\"RMSE\"] + 0.002, i, f\"p={fmt_p(p)}\", va=\"center\", fontsize=9)\n",
    "\n",
    "    ax.set_yticks(y)\n",
    "    ax.set_yticklabels(df[\"Model\"], fontsize=11, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"RMSE (active Stage‑B sequence)\")\n",
    "    ax.set_title(f\"Figure R1 – Model comparison + MCS ({setup})\", pad=12)\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    ax.grid(True, axis=\"x\", linestyle=\":\", alpha=0.6)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if filename:\n",
    "        out = FIG_OUT / filename\n",
    "        fig.savefig(out, bbox_inches=\"tight\")\n",
    "        print(\"Saved:\", out)\n",
    "    plt.show()\n",
    "\n",
    "for s in tables_r1.keys():\n",
    "    plot_mcs_lollipop_for_setup(s, filename=f\"Figure_R1_Lollipop_{s.replace(' ','_')}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33905046",
   "metadata": {},
   "source": [
    "## 7) Setup‑Vergleich über Familien: DM‑Plot mit Setup I als Benchmark\n",
    "Pro Familie wird das beste Modell je Setup (min RMSE) genommen. Dann wird die RMSE‑Änderung vs Setup I (%) geplottet inkl. one‑sided DM‑Signifikanz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881597c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def best_run_per_family_and_setup() -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for r in MODEL_REGISTRY:\n",
    "        rid = r[\"run_id\"]\n",
    "        if rid not in stageb_active:\n",
    "            continue\n",
    "        m = compute_overall_metrics(stageb_active[rid])\n",
    "        rows.append(dict(\n",
    "            run_id=rid, label=r[\"label\"], family=r[\"family\"], setup=r[\"setup\"],\n",
    "            rmse=m[\"rmse\"], mae=m[\"mae\"], n=m[\"n\"]\n",
    "        ))\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        raise ValueError(\"No models loaded.\")\n",
    "    return df.sort_values(\"rmse\").groupby([\"family\", \"setup\"], as_index=False).first()\n",
    "\n",
    "df_best = best_run_per_family_and_setup()\n",
    "df_best\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bfa062",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_setup_gain_vs_setupI(df_best: pd.DataFrame, filename: str | None = None) -> pd.DataFrame:\n",
    "    ref = df_best[df_best[\"setup\"] == \"Setup I\"][[\"family\", \"run_id\", \"rmse\"]].rename(\n",
    "        columns={\"run_id\":\"run_id_ref\", \"rmse\":\"rmse_ref\"}\n",
    "    )\n",
    "    df = df_best.merge(ref, on=\"family\", how=\"inner\")\n",
    "    df[\"pct_change_vs_I\"] = (df[\"rmse\"] - df[\"rmse_ref\"]) / df[\"rmse_ref\"] * 100.0\n",
    "\n",
    "    pvals, sigs = [], []\n",
    "    for _, row in df.iterrows():\n",
    "        if row[\"setup\"] == \"Setup I\":\n",
    "            pvals.append(np.nan)\n",
    "            sigs.append(False)\n",
    "            continue\n",
    "        _, p, _ = dm_test_hac(\n",
    "            losses_se(stageb_active[row[\"run_id\"]]),\n",
    "            losses_se(stageb_active[row[\"run_id_ref\"]]),\n",
    "            alternative=\"less\"  # H1: setup run better than setup I\n",
    "        )\n",
    "        pvals.append(p)\n",
    "        sigs.append(p < ALPHA_SIG)\n",
    "\n",
    "    df[\"dm_p_vs_I\"] = pvals\n",
    "    df[\"sig_vs_I\"] = sigs\n",
    "\n",
    "    fam_order = (\n",
    "        df[df[\"setup\"] != \"Setup I\"]\n",
    "          .sort_values(\"pct_change_vs_I\", ascending=True)[\"family\"]\n",
    "          .unique()\n",
    "          .tolist()\n",
    "    )\n",
    "    y_map = {f:i for i,f in enumerate(fam_order)}\n",
    "\n",
    "    marker_map = {\"Setup II\":\"o\", \"Setup III\":\"D\"}\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 0.6*len(fam_order)+2), dpi=120)\n",
    "    ax.axvline(0, color=\"black\", linewidth=1.5)\n",
    "    ax.grid(True, axis=\"x\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "    for _, row in df[df[\"setup\"] != \"Setup I\"].iterrows():\n",
    "        yy = y_map[row[\"family\"]]\n",
    "        x = row[\"pct_change_vs_I\"]\n",
    "        m = marker_map.get(row[\"setup\"], \"o\")\n",
    "        face = \"black\" if row[\"sig_vs_I\"] else \"white\"\n",
    "        ax.hlines(yy, 0, x, alpha=0.20, linewidth=2)\n",
    "        ax.plot(x, yy, marker=m, markersize=9, markerfacecolor=face, markeredgecolor=\"black\", markeredgewidth=1.8, linestyle=\"\")\n",
    "        ax.text(x + (0.8 if x>=0 else -0.8), yy, f\"{row['setup']}: p={fmt_p(row['dm_p_vs_I'])}\",\n",
    "                va=\"center\", ha=\"left\" if x>=0 else \"right\", fontsize=9)\n",
    "\n",
    "    ax.set_yticks(range(len(fam_order)))\n",
    "    ax.set_yticklabels(fam_order, fontsize=11, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"RMSE change vs Setup I (%)  (negative = improvement)\")\n",
    "    ax.set_title(\"Setup comparison per family (best per setup; DM vs Setup I)\", pad=12)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if filename:\n",
    "        out = FIG_OUT / filename\n",
    "        fig.savefig(out, bbox_inches=\"tight\")\n",
    "        print(\"Saved:\", out)\n",
    "\n",
    "    plt.show()\n",
    "    return df\n",
    "\n",
    "df_setup_gain = plot_setup_gain_vs_setupI(df_best, filename=\"DM_Plot_Setup_Comparison_vs_SetupI.png\")\n",
    "df_setup_gain.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bd9ed7",
   "metadata": {},
   "source": [
    "## 8) Figure R2 (pro Setup): 24M Rolling RMSE – pro Familie (best run)\n",
    "(Eher Appendix, aber hier direkt erzeugt.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c872067",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rolling_rmse_from_preds(df_pred: pd.DataFrame, window: int = ROLLING_WINDOW_MONTHS) -> pd.Series:\n",
    "    se = losses_se(df_pred)\n",
    "    return (se.rolling(window=window, min_periods=window).mean() ** 0.5).rename(f\"roll_rmse_{window}\")\n",
    "\n",
    "def plot_rolling_rmse_by_family_per_setup(df_best: pd.DataFrame, window: int = ROLLING_WINDOW_MONTHS, filename_prefix: str = \"Figure_R2_RollingRMSE\"):\n",
    "    for setup in SETUP_ORDER:\n",
    "        sub = df_best[df_best[\"setup\"] == setup].copy()\n",
    "        if sub.empty:\n",
    "            continue\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(11, 6), dpi=120)\n",
    "        for _, row in sub.iterrows():\n",
    "            rid = row[\"run_id\"]\n",
    "            fam = row[\"family\"]\n",
    "            rr = rolling_rmse_from_preds(stageb_active[rid], window=window)\n",
    "            ax.plot(rr.index, rr.values, label=fam)\n",
    "\n",
    "        ax.set_title(f\"{window}M rolling RMSE by family (best run) – {setup}\")\n",
    "        ax.set_ylabel(\"Rolling RMSE\")\n",
    "        ax.set_xlabel(\"Date\")\n",
    "        ax.legend(ncol=2, frameon=True)\n",
    "        ax.grid(True, axis=\"y\", linestyle=\":\", alpha=0.6)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        out = FIG_OUT / f\"{filename_prefix}_{setup.replace(' ','_')}.png\"\n",
    "        fig.savefig(out, bbox_inches=\"tight\")\n",
    "        print(\"Saved:\", out)\n",
    "        plt.show()\n",
    "\n",
    "plot_rolling_rmse_by_family_per_setup(df_best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd42e525",
   "metadata": {},
   "source": [
    "## 9) Baselines: Table R3 + Relative RMSE Matrix (Appendix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9aa325",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_table_r3_baselines() -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for r in BASELINE_REGISTRY:\n",
    "        rid = r[\"run_id\"]\n",
    "        if rid not in stageb_active:\n",
    "            continue\n",
    "        m = compute_overall_metrics(stageb_active[rid])\n",
    "        rows.append(dict(run_id=rid, Model=r[\"label\"], RMSE=m[\"rmse\"], MAE=m[\"mae\"], N=m[\"n\"]))\n",
    "    return pd.DataFrame(rows).sort_values(\"RMSE\")\n",
    "\n",
    "try:\n",
    "    table_r3 = build_table_r3_baselines()\n",
    "    out_csv = TAB_OUT / \"Table_R3_Baselines.csv\"\n",
    "    table_r3.to_csv(out_csv, index=False)\n",
    "    display(table_r3.style.format({\"RMSE\":\"{:.3f}\", \"MAE\":\"{:.3f}\"}))\n",
    "    print(\"Saved:\", out_csv)\n",
    "\n",
    "    BEST_BASELINE_RID = table_r3.iloc[0][\"run_id\"]\n",
    "    BEST_BASELINE_LABEL = table_r3.iloc[0][\"Model\"]\n",
    "    print(f\"Best baseline: {BEST_BASELINE_LABEL} (run_id={BEST_BASELINE_RID})\")\n",
    "except Exception as e:\n",
    "    print(\"[SKIP] Baselines:\", e)\n",
    "    table_r3 = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68ff251",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def relative_rmse_matrix(models_registry, baselines_registry) -> pd.DataFrame:\n",
    "    model_rows = [r for r in models_registry if r[\"run_id\"] in stageb_active]\n",
    "    base_rows  = [r for r in baselines_registry if r[\"run_id\"] in stageb_active]\n",
    "    if not model_rows or not base_rows:\n",
    "        raise ValueError(\"Need at least 1 model and 1 baseline loaded.\")\n",
    "\n",
    "    model_rmse = {r[\"label\"]: compute_overall_metrics(stageb_active[r[\"run_id\"]])[\"rmse\"] for r in model_rows}\n",
    "\n",
    "    out = pd.DataFrame(index=[b[\"label\"] for b in base_rows], columns=[\"Abs RMSE (Benchmark)\"] + list(model_rmse.keys()), dtype=float)\n",
    "\n",
    "    for b in base_rows:\n",
    "        b_rmse = compute_overall_metrics(stageb_active[b[\"run_id\"]])[\"rmse\"]\n",
    "        out.loc[b[\"label\"], \"Abs RMSE (Benchmark)\"] = b_rmse\n",
    "        for mlabel, m_rmse in model_rmse.items():\n",
    "            out.loc[b[\"label\"], mlabel] = m_rmse / b_rmse\n",
    "\n",
    "    return out\n",
    "\n",
    "try:\n",
    "    rel_mat = relative_rmse_matrix(MODEL_REGISTRY, BASELINE_REGISTRY)\n",
    "    out_csv = TAB_OUT / \"Appendix_Relative_RMSE_Matrix.csv\"\n",
    "    rel_mat.to_csv(out_csv)\n",
    "    display(rel_mat.style.format(\"{:.3f}\"))\n",
    "    print(\"Saved:\", out_csv)\n",
    "except Exception as e:\n",
    "    print(\"[SKIP] Relative RMSE matrix:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff15847c",
   "metadata": {},
   "source": [
    "## 10) Baselines: one‑sided DM je Benchmark + Holm‑Bonferroni (FWER)\n",
    "Pro Familie wird die **beste Kombination über alle Setups** gewählt (min RMSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eede7194",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def best_run_per_family_overall(df_best: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_best.sort_values(\"rmse\").groupby(\"family\", as_index=False).first()\n",
    "    df[\"Model_Label\"] = df.apply(lambda r: f\"{r['family']} ({r['setup'].replace('Setup ','S')})\", axis=1)\n",
    "    return df\n",
    "\n",
    "def plot_dm_vs_each_baseline_with_holm(df_best: pd.DataFrame, filename: str | None = None):\n",
    "    base_rows = [r for r in BASELINE_REGISTRY if r[\"run_id\"] in stageb_active]\n",
    "    if not base_rows:\n",
    "        raise ValueError(\"No baselines loaded.\")\n",
    "\n",
    "    fam_best = best_run_per_family_overall(df_best)\n",
    "    panels = len(base_rows)\n",
    "\n",
    "    fig, axes = plt.subplots(1, panels, figsize=(6*panels, 6), dpi=120)\n",
    "    if panels == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, b in zip(axes, base_rows):\n",
    "        b_rid = b[\"run_id\"]\n",
    "        b_label = b[\"label\"]\n",
    "        b_rmse = compute_overall_metrics(stageb_active[b_rid])[\"rmse\"]\n",
    "\n",
    "        p_raw = []\n",
    "        rows = []\n",
    "        for _, r in fam_best.iterrows():\n",
    "            rid = r[\"run_id\"]\n",
    "            # H1: model better => loss(model) < loss(baseline)\n",
    "            _, p, _ = dm_test_hac(losses_se(stageb_active[rid]), losses_se(stageb_active[b_rid]), alternative=\"less\")\n",
    "            p_raw.append(p)\n",
    "            rows.append(dict(Model_Label=r[\"Model_Label\"], RMSE=r[\"rmse\"], P_raw=p))\n",
    "\n",
    "        reject, p_adj, _, _ = multipletests(p_raw, alpha=ALPHA_SIG, method=\"holm\")\n",
    "        dfp = pd.DataFrame(rows)\n",
    "        dfp[\"P_adj\"] = p_adj\n",
    "        dfp[\"Significant\"] = reject\n",
    "        dfp = dfp.sort_values(\"RMSE\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "        y = np.arange(len(dfp))\n",
    "        ax.axvline(b_rmse, color=\"black\", linewidth=2, alpha=0.8)\n",
    "        ax.hlines(y=y, xmin=dfp[\"RMSE\"], xmax=b_rmse, alpha=0.25, linewidth=1)\n",
    "\n",
    "        for i, row in dfp.iterrows():\n",
    "            better = row[\"RMSE\"] < b_rmse\n",
    "            sig = bool(row[\"Significant\"]) and better\n",
    "            marker = \"D\" if sig else \"o\"\n",
    "            face = \"black\" if sig else \"white\"\n",
    "            ax.plot(row[\"RMSE\"], i, marker=marker, markersize=9,\n",
    "                    markerfacecolor=face, markeredgecolor=\"black\", markeredgewidth=1.5, linestyle=\"\")\n",
    "            ax.text(row[\"RMSE\"] - 0.002, i, f\"p_adj={fmt_p(row['P_adj'])}\", va=\"center\", ha=\"right\", fontsize=9)\n",
    "\n",
    "        ax.set_yticks(y)\n",
    "        ax.set_yticklabels(dfp[\"Model_Label\"], fontsize=10, fontweight=\"bold\")\n",
    "        ax.set_title(f\"vs. {b_label}\")\n",
    "        ax.set_xlabel(\"RMSE (lower is better)\")\n",
    "        ax.grid(True, axis=\"x\", linestyle=\":\", alpha=0.6)\n",
    "\n",
    "    plt.suptitle(\"One-sided pairwise DM tests vs baselines (Holm-Bonferroni across families)\", y=1.02)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if filename:\n",
    "        out = FIG_OUT / filename\n",
    "        fig.savefig(out, bbox_inches=\"tight\")\n",
    "        print(\"Saved:\", out)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_dm_vs_each_baseline_with_holm(df_best, filename=\"Baselines_DM_Holm_Panels.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47b05fd",
   "metadata": {},
   "source": [
    "## 11) Online Policy Timeline (Appendix)\n",
    "Zeigt aktive `config_id` über die Zeit + Switch‑Marker + `wrmse_window`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe8fe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_stageb_scores(run_id: str) -> pd.DataFrame:\n",
    "    path = STAGEB_DIR / run_id / \"monthly\" / \"scores.csv\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    return pd.read_csv(path)\n",
    "\n",
    "def plot_incumbent_timeline(run_id: str, title: str | None = None, filename: str | None = None):\n",
    "    df = load_stageb_scores(run_id).copy()\n",
    "    df = df.sort_values([\"t\", \"config_id\"]).groupby(\"t\", as_index=False).first()\n",
    "\n",
    "    # map t -> date from preds file (for nicer axis)\n",
    "    dfp = pd.read_csv(_stageb_preds_path(run_id), parse_dates=[\"date_t_plus_1\"]).sort_values(\"date_t_plus_1\")\n",
    "    t_to_date = dfp.drop_duplicates(\"t\").set_index(\"t\")[\"date_t_plus_1\"]\n",
    "    df[\"date\"] = df[\"t\"].map(t_to_date)\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(11, 4), dpi=120)\n",
    "    ax1.plot(df[\"date\"], df[\"active_idx\"], drawstyle=\"steps-mid\", linewidth=2)\n",
    "    ax1.set_ylabel(\"Active config_id\")\n",
    "    ax1.set_xlabel(\"Date\")\n",
    "    ax1.grid(True, axis=\"y\", linestyle=\":\", alpha=0.6)\n",
    "\n",
    "    switches = df[df[\"switched\"] == True]\n",
    "    ax1.scatter(switches[\"date\"], switches[\"active_idx\"], s=60, marker=\"D\")\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df[\"date\"], df[\"wrmse_window\"], linestyle=\"--\", linewidth=1.5, alpha=0.8)\n",
    "    ax2.set_ylabel(\"WRMSE window (active)\")\n",
    "\n",
    "    ax1.set_title(title or f\"Incumbent timeline – {run_id}\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if filename:\n",
    "        out = FIG_OUT / filename\n",
    "        fig.savefig(out, bbox_inches=\"tight\")\n",
    "        print(\"Saved:\", out)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "SELECTED_RUN_IDS = [r[\"run_id\"] for r in MODEL_REGISTRY[:2] if r[\"run_id\"] in stageb_active]\n",
    "for rid in SELECTED_RUN_IDS:\n",
    "    try:\n",
    "        plot_incumbent_timeline(rid, filename=f\"Appendix_IncumbentTimeline_{rid}.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"[SKIP] timeline for {rid}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee735cb",
   "metadata": {},
   "source": [
    "## 12) LGBM‑Spezialfall (Setup I): „alle ifo“ vs „nicht alle ifo“ (nur Text/DM)\n",
    "Trage unten zwei Run‑IDs ein und der one‑sided DM‑Test wird ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e56d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EDIT THIS (optional)\n",
    "LGBM_SETUP1_ALL_IFO_RUN = \"LGBM_all_ifo_setup1\"\n",
    "LGBM_SETUP1_SUBSET_IFO_RUN = \"LGBM_subset_ifo_setup1\"\n",
    "\n",
    "def report_dm_two_runs(run_a: str, run_b: str, label_a: str, label_b: str, one_sided_a_better: bool = True):\n",
    "    if run_a not in stageb_active or run_b not in stageb_active:\n",
    "        print(\"[SKIP] One of the runs is not loaded.\")\n",
    "        return\n",
    "\n",
    "    la = losses_se(stageb_active[run_a])\n",
    "    lb = losses_se(stageb_active[run_b])\n",
    "\n",
    "    alt = \"less\" if one_sided_a_better else \"two-sided\"\n",
    "    # test if A is better than B (lower loss)\n",
    "    t, p, n = dm_test_hac(la, lb, alternative=alt)\n",
    "\n",
    "    m_a = compute_overall_metrics(stageb_active[run_a])\n",
    "    m_b = compute_overall_metrics(stageb_active[run_b])\n",
    "\n",
    "    print(f\"DM test: {label_a} vs {label_b}  (alt={alt}, HAC={HAC_LAGS}, n={n})\")\n",
    "    print(f\"  RMSE({label_a})={m_a['rmse']:.3f} | RMSE({label_b})={m_b['rmse']:.3f}\")\n",
    "    print(f\"  t={t:.3f}, p={p:.4f}\")\n",
    "\n",
    "report_dm_two_runs(\n",
    "    LGBM_SETUP1_ALL_IFO_RUN,\n",
    "    LGBM_SETUP1_SUBSET_IFO_RUN,\n",
    "    label_a=\"LGBM (all ifo, S1)\",\n",
    "    label_b=\"LGBM (subset ifo, S1)\",\n",
    "    one_sided_a_better=True\n",
    ")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## EN: Screening 7000 vs 700 (Setup I) – one-sided DM (is 7000 better?)\n",
   "id": "e387f5c9891bd081"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-20T08:51:32.537108Z",
     "start_time": "2026-01-20T08:51:32.407748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "EN_700_RUN = \"elastic_net_with_target_700\"\n",
    "EN_7000_RUN = \"elastic_net_with_targetfeatures_7000\"\n",
    "\n",
    "# Ensure both are loaded (reload only if missing)\n",
    "for rid in [EN_700_RUN, EN_7000_RUN]:\n",
    "    if rid not in stageb_active:\n",
    "        try:\n",
    "            stageb_active[rid] = load_active_stageb_predictions(rid)\n",
    "            print(f\"[OK] loaded: {rid}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[FAIL] {rid}: {e}\")\n",
    "\n",
    "# H1: 7000 better than 700  => mean(loss_7000 - loss_700) < 0\n",
    "report_dm_two_runs(\n",
    "    run_a=EN_7000_RUN,\n",
    "    run_b=EN_700_RUN,\n",
    "    label_a=\"Elastic Net (7000 features)\",\n",
    "    label_b=\"Elastic Net (700 features)\",\n",
    "    one_sided_a_better=True\n",
    ")\n"
   ],
   "id": "9936a49d789c0037",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stageb_active' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 8\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Ensure both are loaded (reload only if missing)\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m rid \u001B[38;5;129;01min\u001B[39;00m [EN_700_RUN, EN_7000_RUN]:\n\u001B[0;32m----> 8\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m rid \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[43mstageb_active\u001B[49m:\n\u001B[1;32m      9\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     10\u001B[0m             stageb_active[rid] \u001B[38;5;241m=\u001B[39m load_active_stageb_predictions(rid)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'stageb_active' is not defined"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
