{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-25T07:49:10.949302Z",
     "start_time": "2026-01-25T07:49:06.670784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# =============================================================================\n",
    "# Stage-A Ensemble Tuning: tune on Block 1+2, evaluate on Block 3 (PRINT ONLY)\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# --- Projekt-Root finden ---\n",
    "def _locate_repo_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    for _ in range(6):\n",
    "        if (cur / \"src\").exists():\n",
    "            return cur\n",
    "        if cur.parent == cur:\n",
    "            break\n",
    "        cur = cur.parent\n",
    "    return start.resolve()\n",
    "\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = _locate_repo_root(NOTEBOOK_DIR)\n",
    "os.environ[\"PROJECT_ROOT\"] = str(PROJECT_ROOT)\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.config import outputs_for_model\n",
    "from src.evaluation import rmse\n",
    "from src.models.ensemble import (\n",
    "    equal_weight_ensemble,\n",
    "    trimmed_mean_ensemble,\n",
    "    median_ensemble,\n",
    "    fit_stacking_ensemble,\n",
    "    fit_ewa_ensemble,\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "BASE_MODELS = [\n",
    "    \"elastic_net_dynamic_fi\",\n",
    "    \"lightgbm_dynamic_fi\",\n",
    "    \"svr_dynamic_fi\",\n",
    "    \"tabpfn_dynamic_fi\",\n",
    "]\n",
    "\n",
    "TRIM_ALPHAS = [0.00, 0.05, 0.10, 0.15, 0.20]\n",
    "STACK_LAMBDAS = [1e-3, 1e-2, 1e-1, 1.0, 10.0]\n",
    "EWA_ETAS = [0.05, 0.10, 0.20, 0.30, 0.50, 0.80, 1.0, 2.0]\n",
    "EWA_DELTA = 0.95\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# HELPERS: load Stage-A block preds (champion per block)\n",
    "# ----------------------------\n",
    "def load_stageA_block_champion_preds(model_name: str, block: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads Stage-A block{block} preds for the best config in that block (lowest rmse in rmse.csv).\n",
    "    Returns DataFrame indexed by date_t_plus_1 with columns: y_true, <model_name>\n",
    "    \"\"\"\n",
    "    outs = outputs_for_model(model_name)\n",
    "    block_dir = outs[\"stageA\"] / f\"block{block}\"\n",
    "    preds_path = block_dir / \"preds.csv\"\n",
    "    rmse_path = block_dir / \"rmse.csv\"\n",
    "\n",
    "    if not preds_path.exists() or not rmse_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing StageA block{block} files for {model_name}: {preds_path} / {rmse_path}\")\n",
    "\n",
    "    df_rmse = pd.read_csv(rmse_path)\n",
    "    rmse_col = \"rmse\" if \"rmse\" in df_rmse.columns else (\"rmse_val\" if \"rmse_val\" in df_rmse.columns else None)\n",
    "    if rmse_col is None or \"config_id\" not in df_rmse.columns:\n",
    "        raise ValueError(f\"Unexpected rmse.csv format in {rmse_path}\")\n",
    "\n",
    "    best_cfg = df_rmse.sort_values(rmse_col).iloc[0][\"config_id\"]\n",
    "\n",
    "    df = pd.read_csv(preds_path)\n",
    "    if \"date_t_plus_1\" not in df.columns or \"config_id\" not in df.columns:\n",
    "        raise ValueError(f\"Unexpected preds.csv format in {preds_path}\")\n",
    "\n",
    "    df[\"date_t_plus_1\"] = pd.to_datetime(df[\"date_t_plus_1\"])\n",
    "    df = df[df[\"config_id\"] == best_cfg].copy()\n",
    "    df = df.sort_values(\"date_t_plus_1\").set_index(\"date_t_plus_1\")\n",
    "\n",
    "    if df.index.duplicated().any():\n",
    "        df = df[~df.index.duplicated(keep=\"last\")]\n",
    "\n",
    "    if \"y_true\" not in df.columns or \"y_pred\" not in df.columns:\n",
    "        raise ValueError(f\"preds.csv missing y_true/y_pred in {preds_path}\")\n",
    "\n",
    "    return df[[\"y_true\", \"y_pred\"]].rename(columns={\"y_pred\": model_name})\n",
    "\n",
    "\n",
    "def align_pool(dfs: dict, model_names: list) -> tuple[pd.Series, pd.DataFrame, pd.DatetimeIndex]:\n",
    "    common = None\n",
    "    for d in dfs.values():\n",
    "        common = d.index if common is None else common.intersection(d.index)\n",
    "    if common is None or len(common) == 0:\n",
    "        raise ValueError(\"No common dates across models.\")\n",
    "    common = common.sort_values()\n",
    "\n",
    "    first = list(dfs.keys())[0]\n",
    "    y = dfs[first].loc[common, \"y_true\"].copy()\n",
    "    y.name = \"y_true\"\n",
    "\n",
    "    F = pd.DataFrame(index=common)\n",
    "    for m in model_names:\n",
    "        F[m] = dfs[m].loc[common, m].astype(float)\n",
    "\n",
    "    return y, F, common\n",
    "\n",
    "\n",
    "def load_block_pool(block: int, model_names: list) -> tuple[pd.Series, pd.DataFrame, pd.DatetimeIndex]:\n",
    "    dfs = {}\n",
    "    for m in model_names:\n",
    "        dfs[m] = load_stageA_block_champion_preds(m, block)\n",
    "    return align_pool(dfs, model_names)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# LOAD Stage A blocks\n",
    "# ----------------------------\n",
    "print(f\"PROJECT_ROOT: {PROJECT_ROOT}\")\n",
    "\n",
    "y1, F1, d1 = load_block_pool(1, BASE_MODELS)\n",
    "y2, F2, d2 = load_block_pool(2, BASE_MODELS)\n",
    "y3, F3, d3 = load_block_pool(3, BASE_MODELS)\n",
    "\n",
    "# Tune set = Block1+2 (concatenate then align on shared columns; dates are disjoint anyway)\n",
    "y_val = pd.concat([y1, y2]).sort_index()\n",
    "F_val = pd.concat([F1, F2]).sort_index()\n",
    "\n",
    "# Full for EWA run (val + block3), so it can arrive at block3 \"trained\"\n",
    "y_full = pd.concat([y_val, y3]).sort_index()\n",
    "F_full = pd.concat([F_val, F3]).sort_index()\n",
    "\n",
    "val_dates = list(y_val.index)\n",
    "test_dates = list(y3.index)\n",
    "\n",
    "print(\"\\nStage A blocks loaded:\")\n",
    "print(f\"  Block1: {d1.min().date()} -> {d1.max().date()} (n={len(d1)})\")\n",
    "print(f\"  Block2: {d2.min().date()} -> {d2.max().date()} (n={len(d2)})\")\n",
    "print(f\"  Block3: {d3.min().date()} -> {d3.max().date()} (n={len(d3)})\")\n",
    "print(f\"  VAL (B1+B2): n={len(y_val)} | TEST (B3): n={len(y3)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# TUNE + EVAL\n",
    "# ----------------------------\n",
    "results = []\n",
    "\n",
    "\n",
    "def add_result(name: str, params: dict, yhat_val: pd.Series, yhat_test: pd.Series):\n",
    "    r_val = rmse(y_val.values, yhat_val.loc[y_val.index].values)\n",
    "    r_test = rmse(y3.values, yhat_test.loc[y3.index].values)\n",
    "    results.append((name, params, r_val, r_test))\n",
    "\n",
    "\n",
    "# Equal / Median (no params)\n",
    "yhat_equal_val = equal_weight_ensemble(F_val)\n",
    "yhat_equal_test = equal_weight_ensemble(F3)\n",
    "add_result(\"Equal-Weight\", {}, yhat_equal_val, yhat_equal_test)\n",
    "\n",
    "yhat_median_val = median_ensemble(F_val)\n",
    "yhat_median_test = median_ensemble(F3)\n",
    "add_result(\"Median\", {}, yhat_median_val, yhat_median_test)\n",
    "\n",
    "# Trimmed mean: tune alpha on VAL\n",
    "best_alpha, best_r = None, np.inf\n",
    "for a in TRIM_ALPHAS:\n",
    "    yh = trimmed_mean_ensemble(F_val, alpha=a)\n",
    "    r = rmse(y_val.values, yh.values)\n",
    "    if r < best_r:\n",
    "        best_r, best_alpha = r, a\n",
    "\n",
    "yhat_trim_val = trimmed_mean_ensemble(F_val, alpha=best_alpha)\n",
    "yhat_trim_test = trimmed_mean_ensemble(F3, alpha=best_alpha)\n",
    "add_result(\"Trimmed-Mean\", {\"alpha\": best_alpha}, yhat_trim_val, yhat_trim_test)\n",
    "\n",
    "# Stacking: tune lambda (and weights) on VAL, then apply to Block3\n",
    "stack_res = fit_stacking_ensemble(\n",
    "    y=y_val,\n",
    "    F=F_val,\n",
    "    cal_dates=val_dates,\n",
    "    lambdas=STACK_LAMBDAS\n",
    ")\n",
    "w = stack_res.weights.values\n",
    "yhat_stack_val = stack_res.y_pred.loc[y_val.index]\n",
    "yhat_stack_test = pd.Series(F3.values @ w, index=F3.index, name=\"stacked\")\n",
    "add_result(\"Stacking\", {\"lambda\": float(stack_res.lambda_opt)}, yhat_stack_val, yhat_stack_test)\n",
    "\n",
    "# EWA: tune eta on VAL, run online over (VAL+Block3), evaluate on Block3\n",
    "ewa_res = fit_ewa_ensemble(\n",
    "    y=y_full,\n",
    "    F=F_full,\n",
    "    cal_dates=val_dates,\n",
    "    etas=EWA_ETAS,\n",
    "    delta=EWA_DELTA\n",
    ")\n",
    "yhat_ewa_val = ewa_res.y_pred.loc[y_val.index]\n",
    "yhat_ewa_test = ewa_res.y_pred.loc[y3.index]\n",
    "add_result(\"EWA\", {\"eta\": float(ewa_res.eta_opt), \"delta\": float(ewa_res.delta)}, yhat_ewa_val, yhat_ewa_test)\n",
    "\n",
    "# ----------------------------\n",
    "# PRINT SUMMARY\n",
    "# ----------------------------\n",
    "print(\"\\n\" + \"=\" * 72)\n",
    "print(\"TUNING on Block1+2, EVALUATION on Block3\")\n",
    "print(\"=\" * 72)\n",
    "\n",
    "# Pretty print\n",
    "results_sorted = sorted(results, key=lambda x: x[3])  # sort by Block3 RMSE\n",
    "for name, params, r_val, r_test in results_sorted:\n",
    "    p = \", \".join([f\"{k}={v}\" for k, v in params.items()]) if params else \"-\"\n",
    "    print(f\"{name:<14} | params: {p:<20} | RMSE VAL(B1+2): {r_val:.4f} | RMSE TEST(B3): {r_test:.4f}\")\n",
    "\n",
    "best = results_sorted[0]\n",
    "print(\"\\nBest on Block3:\")\n",
    "print(f\"  {best[0]}  (params: {best[1]})  -> RMSE(B3) = {best[3]:.4f}\")\n",
    "\n",
    "print(\"\\n(Stacking weights from VAL):\")\n",
    "print(stack_res.weights)\n",
    "\n",
    "print(\"\\nDone.\")\n"
   ],
   "id": "22939675a898b100",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /Users/jonasschernich/Documents/Masterarbeit/Code\n",
      "\n",
      "Stage A blocks loaded:\n",
      "  Block1: 2006-03-01 -> 2007-10-01 (n=20)\n",
      "  Block2: 2007-11-01 -> 2009-06-01 (n=20)\n",
      "  Block3: 2009-07-01 -> 2011-02-01 (n=20)\n",
      "  VAL (B1+B2): n=40 | TEST (B3): n=20\n",
      "\n",
      "========================================================================\n",
      "TUNING on Block1+2, EVALUATION on Block3\n",
      "========================================================================\n",
      "Equal-Weight   | params: -                    | RMSE VAL(B1+2): 1.9974 | RMSE TEST(B3): 1.5429\n",
      "Trimmed-Mean   | params: alpha=0.0            | RMSE VAL(B1+2): 1.9974 | RMSE TEST(B3): 1.5429\n",
      "Median         | params: -                    | RMSE VAL(B1+2): 1.9981 | RMSE TEST(B3): 1.5513\n",
      "EWA            | params: eta=2.0, delta=0.95  | RMSE VAL(B1+2): 1.9970 | RMSE TEST(B3): 1.5584\n",
      "Stacking       | params: lambda=0.001         | RMSE VAL(B1+2): 1.9877 | RMSE TEST(B3): 1.6107\n",
      "\n",
      "Best on Block3:\n",
      "  Equal-Weight  (params: {})  -> RMSE(B3) = 1.5429\n",
      "\n",
      "(Stacking weights from VAL):\n",
      "elastic_net_dynamic_fi    1.054712e-15\n",
      "lightgbm_dynamic_fi       0.000000e+00\n",
      "svr_dynamic_fi            1.000000e+00\n",
      "tabpfn_dynamic_fi         0.000000e+00\n",
      "Name: weights, dtype: float64\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-17T18:48:19.351807Z",
     "start_time": "2026-01-17T18:48:19.203129Z"
    }
   },
   "source": [
    "# # ==============================================================================\n",
    "# Masterarbeit Ensemble Tuning & Evaluation (Full Timeline 2011-2024)\n",
    "# ==============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Projekt-Root finden ---\n",
    "def _locate_repo_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    for _ in range(5):\n",
    "        if (cur / \"src\").exists(): return cur\n",
    "        if cur.parent == cur: break\n",
    "        cur = cur.parent\n",
    "    return start.resolve()\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = _locate_repo_root(NOTEBOOK_DIR)\n",
    "os.environ[\"PROJECT_ROOT\"] = str(PROJECT_ROOT)\n",
    "if str(PROJECT_ROOT) not in sys.path: sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Imports\n",
    "from src.config import GlobalConfig\n",
    "from src.evaluation import rmse, mae\n",
    "from src.models.ensemble import (\n",
    "    load_level0_pool,\n",
    "    load_calibration_pool,\n",
    "    equal_weight_ensemble,\n",
    "    trimmed_mean_ensemble,\n",
    "    median_ensemble,\n",
    "    fit_stacking_ensemble,\n",
    "    fit_ewa_ensemble,\n",
    ")\n",
    "\n",
    "print(f\"Projekt-Root: {PROJECT_ROOT}\")\n",
    "\n",
    "# --- KONFIGURATION ---\n",
    "# Die exakten Ordnernamen unter outputs/stageB/\n",
    "BASE_MODELS = [\n",
    "    \"elastic_net_dynamic_fi\",\n",
    "    \"lightgbm_dynamic_fi\",\n",
    "    \"svr_dynamic_fi\",\n",
    "    \"tabpfn_dynamic_fi\",\n",
    "]\n",
    "\n",
    "STACKING_LAMBDAS = [1e-2, 0.1, 1.0]\n",
    "EWA_ETAS = [0.1, 0.3, 0.2, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 3]\n",
    "EWA_DELTA = 0.95\n",
    "\n",
    "OUT_DIR = PROJECT_ROOT / \"outputs\" / \"ensembles\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- DATEN LADEN (Stage A + Stage B) ---\n",
    "\n",
    "print(\"\\n1. Lade Stage B (Test Set)...\")\n",
    "try:\n",
    "    pool_test = load_level0_pool(BASE_MODELS)\n",
    "    F_test = pool_test.F\n",
    "    y_test = pool_test.y_true\n",
    "    dates_test = pool_test.dates\n",
    "    print(f\"   Test Period: {dates_test[0].date()} bis {dates_test[-1].date()} (n={len(dates_test)})\")\n",
    "except Exception as e:\n",
    "    print(f\"   FEHLER: Konnte Stage B nicht laden: {e}\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n2. Lade Stage A (Calibration Set, Block 3)...\")\n",
    "try:\n",
    "    pool_cal = load_calibration_pool(BASE_MODELS)\n",
    "    F_cal = pool_cal.F\n",
    "    y_cal = pool_cal.y_true\n",
    "    dates_cal = pool_cal.dates\n",
    "    print(f\"   Cal Period:  {dates_cal[0].date()} bis {dates_cal[-1].date()} (n={len(dates_cal)})\")\n",
    "\n",
    "    # Merge für Training (Historie A + B)\n",
    "    F_full = pd.concat([F_cal, F_test])\n",
    "    y_full = pd.concat([y_cal, y_test])\n",
    "    # Sicherstellen, dass Index sortiert ist\n",
    "    F_full.sort_index(inplace=True)\n",
    "    y_full.sort_index(inplace=True)\n",
    "\n",
    "    cal_dates_list = list(dates_cal) # Wir trainieren NUR auf A\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   WARNUNG: Konnte Stage A nicht laden ({e}).\")\n",
    "    print(\"   Fallback: Nutze die ersten 24 Monate von Stage B als Kalibrierung (Verlust von Testdaten!).\")\n",
    "    split_idx = 24\n",
    "    F_full = F_test\n",
    "    y_full = y_test\n",
    "    cal_dates_list = list(dates_test[:split_idx])\n",
    "    # Für die Evaluation später schneiden wir dann ab\n",
    "\n",
    "# --- ENSEMBLE BERECHNUNG ---\n",
    "\n",
    "print(\"\\n--- Benchmark Ensembles (Equal/Trim/Median) ---\")\n",
    "# Diese brauchen kein Training, können direkt auf Test berechnet werden\n",
    "ens_equal = equal_weight_ensemble(F_test)\n",
    "ens_trim10 = trimmed_mean_ensemble(F_test, alpha=0.10)\n",
    "ens_median = median_ensemble(F_test)\n",
    "\n",
    "print(\"\\n--- Stacking (Learned on Stage A) ---\")\n",
    "stack_res = fit_stacking_ensemble(\n",
    "    y=y_full,\n",
    "    F=F_full,\n",
    "    cal_dates=cal_dates_list,\n",
    "    lambdas=STACKING_LAMBDAS\n",
    ")\n",
    "# Extrahiere Vorhersagen nur für Stage B\n",
    "ens_stack = stack_res.y_pred.loc[dates_test]\n",
    "\n",
    "print(f\"  Best Lambda: {stack_res.lambda_opt}\")\n",
    "print(\"  Weights (from Stage A):\")\n",
    "print(stack_res.weights)\n",
    "\n",
    "print(\"\\n--- EWA (Online Learning A -> B) ---\")\n",
    "# EWA läuft über die ganze Historie (A+B), damit es bei B schon \"schlau\" ist\n",
    "ewa_res = fit_ewa_ensemble(\n",
    "    y=y_full,\n",
    "    F=F_full,\n",
    "    cal_dates=cal_dates_list, # Tuning von Eta auf A\n",
    "    etas=EWA_ETAS,\n",
    "    delta=EWA_DELTA\n",
    ")\n",
    "ens_ewa = ewa_res.y_pred.loc[dates_test]\n",
    "\n",
    "print(f\"  Best Eta: {ewa_res.eta_opt}\")\n",
    "\n",
    "# --- EVALUATION ---\n",
    "\n",
    "df_ens = pd.DataFrame(index=dates_test)\n",
    "df_ens[\"y_true\"] = y_test\n",
    "for m in BASE_MODELS:\n",
    "    df_ens[f\"base_{m}\"] = F_test[m]\n",
    "\n",
    "df_ens[\"ens_equal\"] = ens_equal\n",
    "df_ens[\"ens_stack\"] = ens_stack\n",
    "df_ens[\"ens_ewa\"] = ens_ewa\n",
    "df_ens[\"dispersion\"] = F_test.std(axis=1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"RESULTS (Full Stage B: {dates_test[0].date()} - {dates_test[-1].date()})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def score(y, y_hat, label):\n",
    "    r = rmse(y, y_hat)\n",
    "    print(f\"{label:<25} | RMSE: {r:.4f}\")\n",
    "\n",
    "print(\"--- Base Models ---\")\n",
    "for m in BASE_MODELS:\n",
    "    score(y_test, F_test[m], m)\n",
    "\n",
    "print(\"\\n--- Ensembles ---\")\n",
    "score(y_test, ens_equal, \"Equal-Weight\")\n",
    "score(y_test, ens_stack, \"Stacked (Static)\")\n",
    "score(y_test, ens_ewa, \"EWA (Online)\")\n",
    "\n",
    "# Speichern\n",
    "df_ens.to_csv(OUT_DIR / \"ensemble_predictions.csv\")\n",
    "print(f\"\\nSaved to {OUT_DIR}\")\n",
    "# ==============================================================================\n",
    "# 12. Export als \"Fake\" Stage B Modelle für einfache Vergleichbarkeit\n",
    "# ==============================================================================\n",
    "# Ziel: Erstelle Ordnerstruktur wie bei normalen Modellen:\n",
    "# outputs/stageB/ensemble_stacking/monthly/preds.csv\n",
    "# outputs/stageB/ensemble_ewa/monthly/preds.csv\n",
    "# ...\n",
    "\n",
    "def export_ensemble_to_stageB(\n",
    "    df_results: pd.DataFrame,\n",
    "    col_name: str,\n",
    "    model_export_name: str,\n",
    "    base_dir: Path\n",
    "):\n",
    "    \"\"\"\n",
    "    Speichert eine Ensemble-Spalte im Format der Stage B Predictions.\n",
    "    \"\"\"\n",
    "    # Zielverzeichnis erstellen\n",
    "    model_dir = base_dir / \"stageB\" / model_export_name\n",
    "    monthly_dir = model_dir / \"monthly\"\n",
    "    summary_dir = model_dir / \"summary\"\n",
    "\n",
    "    monthly_dir.mkdir(parents=True, exist_ok=True)\n",
    "    summary_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1. preds.csv erstellen\n",
    "    # Das Format muss exakt den anderen Modellen entsprechen\n",
    "    df_out = pd.DataFrame()\n",
    "    df_out[\"date_t_plus_1\"] = df_results.index\n",
    "    # Wir brauchen eine Dummy 't' Spalte (Origin), nehmen wir einfach Monat davor\n",
    "    df_out[\"t\"] = df_results.index - pd.DateOffset(months=1)\n",
    "    df_out[\"y_true\"] = df_results[\"y_true\"].values\n",
    "    df_out[\"y_pred\"] = df_results[col_name].values\n",
    "\n",
    "    # Metadaten faken, damit die Analysis-Skripte nicht abstürzen\n",
    "    df_out[\"config_id\"] = \"ensemble_v1\" # Dummy Config ID\n",
    "    df_out[\"is_active\"] = True          # Ensembles sind immer \"aktiv\"\n",
    "\n",
    "    # Speichern\n",
    "    out_path = monthly_dir / \"preds.csv\"\n",
    "    df_out.to_csv(out_path, index=False)\n",
    "    print(f\"  Exportiert: {out_path}\")\n",
    "\n",
    "    # 2. summary.csv erstellen (wird oft zum Laden der Configs gesucht)\n",
    "    # Berechne RMSE für dieses Ensemble\n",
    "    mse = ((df_out[\"y_true\"] - df_out[\"y_pred\"])**2).mean()\n",
    "    rmse_val = np.sqrt(mse)\n",
    "\n",
    "    df_sum = pd.DataFrame([{\n",
    "        \"config_id\": \"ensemble_v1\",\n",
    "        \"model\": model_export_name,\n",
    "        \"rmse_overall\": rmse_val, # Gesamt-RMSE\n",
    "        \"setup_name\": \"Ensemble\"\n",
    "    }])\n",
    "    df_sum.to_csv(summary_dir / \"summary.csv\", index=False)\n",
    "\n",
    "print(\"\\n--- Exportiere Ensembles in Stage B Struktur ---\")\n",
    "\n",
    "# Mapping: Spaltenname im DataFrame -> Name des Ordners in outputs/stageB\n",
    "ensembles_to_export = {\n",
    "    \"ens_equal\":  \"ensemble_equal_weight\",\n",
    "    \"ens_trim10\": \"ensemble_trimmed\",\n",
    "    \"ens_median\": \"ensemble_median\",\n",
    "    \"ens_stack\":  \"ensemble_stacking\",\n",
    "    \"ens_ewa\":    \"ensemble_ewa\"\n",
    "}\n",
    "\n",
    "OUTPUTS_ROOT = PROJECT_ROOT / \"outputs\"\n",
    "\n",
    "for col, folder_name in ensembles_to_export.items():\n",
    "    if col in df_ens.columns:\n",
    "        export_ensemble_to_stageB(df_ens, col, folder_name, OUTPUTS_ROOT)\n",
    "\n",
    "print(\"\\nFertig! Die Ensembles sind jetzt bereit für die Vergleichs-Notebooks.\")"
   ],
   "id": "10dae65afde5a7eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projekt-Root: /Users/jonasschernich/Documents/Masterarbeit/Code\n",
      "\n",
      "1. Lade Stage B (Test Set)...\n",
      "   Test Period: 2011-03-01 bis 2024-12-01 (n=166)\n",
      "\n",
      "2. Lade Stage A (Calibration Set, Block 3)...\n",
      "   Cal Period:  2009-07-01 bis 2011-02-01 (n=20)\n",
      "\n",
      "--- Benchmark Ensembles (Equal/Trim/Median) ---\n",
      "\n",
      "--- Stacking (Learned on Stage A) ---\n",
      "  Best Lambda: 0.01\n",
      "  Weights (from Stage A):\n",
      "elastic_net_dynamic_fi    1.000000e+00\n",
      "lightgbm_dynamic_fi       5.606626e-15\n",
      "svr_dynamic_fi            1.454392e-14\n",
      "tabpfn_dynamic_fi         0.000000e+00\n",
      "Name: weights, dtype: float64\n",
      "\n",
      "--- EWA (Online Learning A -> B) ---\n",
      "  Best Eta: 0.1\n",
      "\n",
      "============================================================\n",
      "RESULTS (Full Stage B: 2011-03-01 - 2024-12-01)\n",
      "============================================================\n",
      "--- Base Models ---\n",
      "elastic_net_dynamic_fi    | RMSE: 2.3141\n",
      "lightgbm_dynamic_fi       | RMSE: 2.3706\n",
      "svr_dynamic_fi            | RMSE: 2.3422\n",
      "tabpfn_dynamic_fi         | RMSE: 2.3302\n",
      "\n",
      "--- Ensembles ---\n",
      "Equal-Weight              | RMSE: 2.3175\n",
      "Stacked (Static)          | RMSE: 2.3141\n",
      "EWA (Online)              | RMSE: 2.3170\n",
      "\n",
      "Saved to /Users/jonasschernich/Documents/Masterarbeit/Code/outputs/ensembles\n",
      "\n",
      "--- Exportiere Ensembles in Stage B Struktur ---\n",
      "  Exportiert: /Users/jonasschernich/Documents/Masterarbeit/Code/outputs/stageB/ensemble_equal_weight/monthly/preds.csv\n",
      "  Exportiert: /Users/jonasschernich/Documents/Masterarbeit/Code/outputs/stageB/ensemble_stacking/monthly/preds.csv\n",
      "  Exportiert: /Users/jonasschernich/Documents/Masterarbeit/Code/outputs/stageB/ensemble_ewa/monthly/preds.csv\n",
      "\n",
      "Fertig! Die Ensembles sind jetzt bereit für die Vergleichs-Notebooks.\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
