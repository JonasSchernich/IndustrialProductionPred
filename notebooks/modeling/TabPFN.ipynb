{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T06:53:35.452178Z",
     "start_time": "2025-11-20T20:17:32.665304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# TabPFN – Thesis Tuning Pipeline\n",
    "# ==============================================================================\n",
    "# Spezifikation gemäß Masterarbeit:\n",
    "# - Modell: Pre-Trained (kein HP-Tuning). Fokus liegt auf Input-Optimierung.\n",
    "# - Setup I & II: FE-Grid mit DR-Constraint (TabPFN braucht kompakte Inputs).\n",
    "#   -> Wenn SIS > 300 Features, ist DR (PCA/PLS) Pflicht.\n",
    "# - Setup III: Dynamic FI (Strikt Top 20 Features).\n",
    "# - Stage A Shortlist: Top 10 frozen.\n",
    "# ==============================================================================\n",
    "\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "\n",
    "# --- 1) Pfad-Setup ---\n",
    "def _locate_repo_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    for _ in range(6):\n",
    "        if (cur / \"src\").exists():\n",
    "            return cur\n",
    "        if cur.parent == cur:\n",
    "            break\n",
    "        cur = cur.parent\n",
    "    return start.resolve()\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = _locate_repo_root(NOTEBOOK_DIR)\n",
    "os.environ[\"PROJECT_ROOT\"] = str(PROJECT_ROOT)\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# --- 2) Imports ---\n",
    "from src.config import (\n",
    "    GlobalConfig,\n",
    "    DEFAULT_CORR_SPEC,    # expanding\n",
    "    EWMA_CORR_SPEC,       # ewma\n",
    "    outputs_for_model,\n",
    ")\n",
    "from src.tuning import run_stageA, run_stageB\n",
    "from src.io_timesplits import (\n",
    "    load_target,\n",
    "    load_ifo_features,\n",
    "    load_full_lagged_features,\n",
    "    load_rolling_importance,\n",
    ")\n",
    "# WICHTIG: Modell-Import für TabPFN\n",
    "from src.models.TabPFN import ForecastModel\n",
    "\n",
    "# --- 3) Konfiguration ---\n",
    "USE_DYNAMIC_FI_PIPELINE = False  # False = Standard Setup (I & II)\n",
    "SEED = 42\n",
    "\n",
    "# GPU Einstellungen für TabPFN (CUDA oder MPS)\n",
    "USE_GPU = True\n",
    "FORCE_DEVICE = None # z.B. \"cuda\" oder \"mps\", falls Auto-Detect versagt\n",
    "\n",
    "if USE_DYNAMIC_FI_PIPELINE:\n",
    "    MODEL_NAME = \"tabpfn_dynamic_fi\"\n",
    "else:\n",
    "    MODEL_NAME = \"tabpfn\"\n",
    "\n",
    "outputs_for_model(MODEL_NAME)\n",
    "print(f\"--- Starte Tuning für: {MODEL_NAME} ---\")\n",
    "\n",
    "# --- 4) Daten laden ---\n",
    "y = load_target()\n",
    "X_ifo = load_ifo_features()\n",
    "\n",
    "# Align Indizes\n",
    "idx_common = y.index.intersection(X_ifo.index)\n",
    "y = y.loc[idx_common]\n",
    "X_ifo = X_ifo.loc[idx_common]\n",
    "\n",
    "X_full_lagged = None\n",
    "rolling_imp = None\n",
    "y_fi = None\n",
    "\n",
    "if USE_DYNAMIC_FI_PIPELINE:\n",
    "    FI_BASE_DIR = PROJECT_ROOT / \"outputs\" / \"feature_importance\" / \"outputs_no_missing\"\n",
    "    try:\n",
    "        X_full_lagged = load_full_lagged_features(base_dir=FI_BASE_DIR)\n",
    "        rolling_imp   = load_rolling_importance(base_dir=FI_BASE_DIR)\n",
    "\n",
    "        idx_fi = y.index.intersection(X_full_lagged.index).intersection(rolling_imp.index)\n",
    "        y_fi          = y.loc[idx_fi]\n",
    "        X_full_lagged = X_full_lagged.loc[idx_fi]\n",
    "        rolling_imp   = rolling_imp.loc[idx_fi]\n",
    "        print(f\"Dynamic FI Modus: {X_full_lagged.shape[1]} Features geladen.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"FEHLER: Dynamic FI Artefakte nicht gefunden.\")\n",
    "        sys.exit(1)\n",
    "else:\n",
    "    print(f\"Full FE Modus (Setup I/II): {X_ifo.shape[1]} Basis-Features.\")\n",
    "\n",
    "# --- 5) Config Defaults (Thesis Policy) ---\n",
    "def get_thesis_cfg() -> GlobalConfig:\n",
    "    cfg = GlobalConfig(preset=\"thesis\")\n",
    "    cfg.policy_window = 24\n",
    "    cfg.policy_decay = 0.97\n",
    "    cfg.selection_mode = \"decayed_best\"\n",
    "    return cfg\n",
    "\n",
    "cfg_obj = get_thesis_cfg()\n",
    "\n",
    "# --- 6) Grid Definition ---------------------------------------\n",
    "\n",
    "def build_grid_full_fe():\n",
    "    \"\"\"Setup I (ifo) und Setup II (ifo + TargetBlocks).\"\"\"\n",
    "\n",
    "    # A) FE & DR (Standard Thesis Grid)\n",
    "    lag_candidates = [tuple(range(7))]\n",
    "\n",
    "    corr_opts = [\n",
    "        {\"corr_spec\": dict(DEFAULT_CORR_SPEC)},\n",
    "        {\"corr_spec\": dict(EWMA_CORR_SPEC)},\n",
    "    ]\n",
    "\n",
    "    k1_opts = [300, 5000, 50000]\n",
    "    red_opts = [0.9, 1.0]\n",
    "\n",
    "    dr_opts = [\n",
    "        {\"dr_method\": \"none\"},\n",
    "        {\"dr_method\": \"pca\", \"pca_kmax\": 15, \"pca_var_target\": 0.99},\n",
    "        {\"dr_method\": \"pca\", \"pca_kmax\": 30, \"pca_var_target\": 0.99},\n",
    "        {\"dr_method\": \"pls\", \"pls_components\": 15},\n",
    "        {\"dr_method\": \"pls\", \"pls_components\": 30},\n",
    "    ]\n",
    "\n",
    "    # B) Setup II (Target Blocks)\n",
    "    block_opts = [\n",
    "        None,                                 # Setup I\n",
    "        [\"AR1\", \"Chronos\"],                   # Setup II a\n",
    "        [\"AR1\", \"Chronos\", \"TSFresh\"]         # Setup II b\n",
    "    ]\n",
    "\n",
    "    # C) Weights\n",
    "    # TabPFN unterstützt keine sample_weights (siehe src/models/TabPFN.py).\n",
    "    # Wir setzen nur None, um redundante Berechnungen zu sparen.\n",
    "    weight_opts = [\n",
    "        {\"sample_weight_decay\": None}\n",
    "    ]\n",
    "\n",
    "    grid = []\n",
    "\n",
    "    # 1. FE Loop\n",
    "    for lags, corr, k1, red, dr in product(lag_candidates, corr_opts, k1_opts, red_opts, dr_opts):\n",
    "\n",
    "        # --- TABPFN CONSTRAINT ---\n",
    "        # TabPFN performed am besten mit kleinen Inputs (<100 Features).\n",
    "        # Wenn DR=\"none\" und K1 > 300 (also 5k oder 50k), überspringen wir das.\n",
    "        # Das Modell würde sonst extrem langsam sein oder Context-Limits sprengen.\n",
    "        if dr[\"dr_method\"] == \"none\" and k1 > 300:\n",
    "            continue\n",
    "\n",
    "        base_fe = {\n",
    "            \"lag_candidates\": lags,\n",
    "            \"k1_topk\": k1,\n",
    "            \"redundancy_param\": red,\n",
    "            **dr,\n",
    "            **corr\n",
    "        }\n",
    "\n",
    "        # 2. Blocks & Weights & Model Params\n",
    "        for blocks, weights in product(block_opts, weight_opts):\n",
    "            hp = {\n",
    "                **base_fe,\n",
    "                \"target_block_set\": blocks,\n",
    "                **weights,\n",
    "                # TabPFN spezifisch\n",
    "                \"use_gpu\": USE_GPU,\n",
    "                \"device\": FORCE_DEVICE,\n",
    "                \"seed\": SEED\n",
    "            }\n",
    "            grid.append(hp)\n",
    "\n",
    "    return grid\n",
    "\n",
    "def build_grid_dynamic_fi():\n",
    "    \"\"\"Setup III: Dynamic Feature Importance via strict Top-N.\"\"\"\n",
    "\n",
    "    n_features_list = [20]  # Fix 20 Features\n",
    "\n",
    "    # Auch hier: TabPFN ignoriert Weights -> nur None\n",
    "    weight_opts = [{\"sample_weight_decay\": None}]\n",
    "\n",
    "    grid = []\n",
    "    for n_feat, w in product(n_features_list, weight_opts):\n",
    "        hp = {\n",
    "            \"n_features_to_use\": n_feat,\n",
    "            **w,\n",
    "            \"use_gpu\": USE_GPU,\n",
    "            \"device\": FORCE_DEVICE,\n",
    "            \"seed\": SEED\n",
    "        }\n",
    "        grid.append(hp)\n",
    "    return grid\n",
    "\n",
    "# --- 7) Ausführung --------------------------------------------\n",
    "\n",
    "if USE_DYNAMIC_FI_PIPELINE:\n",
    "    grid = build_grid_dynamic_fi()\n",
    "    print(f\"Dynamic FI Grid Größe (Setup III): {len(grid)} Konfigurationen.\")\n",
    "\n",
    "    shortlist = run_stageA(\n",
    "        model_name=MODEL_NAME,\n",
    "        model_ctor=lambda hp: ForecastModel(hp),\n",
    "        model_grid=grid,\n",
    "        X=X_ifo, # Dummy\n",
    "        y=y_fi,\n",
    "        cfg=cfg_obj,\n",
    "        X_full_lagged=X_full_lagged,\n",
    "        rolling_imp=rolling_imp,\n",
    "        keep_top_k_final=10,\n",
    "        min_survivors_per_block=0\n",
    "    )\n",
    "\n",
    "    run_stageB(\n",
    "        model_name=MODEL_NAME,\n",
    "        model_ctor=lambda hp: ForecastModel(hp),\n",
    "        shortlist=shortlist,\n",
    "        X=X_ifo, # Dummy\n",
    "        y=y_fi,\n",
    "        cfg=cfg_obj,\n",
    "        X_full_lagged=X_full_lagged,\n",
    "        rolling_imp=rolling_imp\n",
    "    )\n",
    "\n",
    "else:\n",
    "    grid = build_grid_full_fe()\n",
    "    print(f\"Full FE Grid Größe (Setup I & II): {len(grid)} Konfigurationen.\")\n",
    "\n",
    "    # Grid ist durch den DR-Constraint moderat.\n",
    "\n",
    "    shortlist = run_stageA(\n",
    "        model_name=MODEL_NAME,\n",
    "        model_ctor=lambda hp: ForecastModel(hp),\n",
    "        model_grid=grid,\n",
    "        X=X_ifo,\n",
    "        y=y,\n",
    "        cfg=cfg_obj,\n",
    "        keep_top_k_final=10,\n",
    "        min_survivors_per_block=0\n",
    "    )\n",
    "\n",
    "    run_stageB(\n",
    "        model_name=MODEL_NAME,\n",
    "        model_ctor=lambda hp: ForecastModel(hp),\n",
    "        shortlist=shortlist,\n",
    "        X=X_ifo,\n",
    "        y=y,\n",
    "        cfg=cfg_obj\n",
    "    )\n",
    "\n",
    "print(\"\\nTuning abgeschlossen.\")"
   ],
   "id": "ea5eef511abadd0b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT = /Users/jonasschernich/Documents/Masterarbeit/Code\n",
      "Modell tabpfn wird getunt.\n",
      "INFO in load_ifo_features: Renaming columns to ensure validity.\n",
      "Full-FE Daten geladen. Shapes: (407, 2160) (407,)\n",
      "Erstelle HP-Grid für 'Full FE' (Setup II) gemäß Thesis-Text ...\n",
      "Optimierte HP-Kombinationen: 312\n",
      "Erstes HP-Set: {'lag_candidates': (0, 1, 2, 3, 4, 5, 6), 'top_k_lags_per_feature': 1, 'use_rm3': False, 'k1_topk': 300, 'redundancy_param': 0.9, 'dr_method': 'none', 'corr_tag': 'expanding', 'corr_spec': {'mode': 'expanding', 'window': None, 'lam': None}, 'seed': 42, 'use_gpu': False, 'device': None, 'target_block_set': ['AR1', 'Chronos', 'TSFresh'], 'sample_weight_decay': None}\n",
      "[Stage A] Using FULL FE (Gleis 1 & 2) pipeline.\n",
      "[Stage A][Block 1] train_end=180, OOS=181-200 | configs=312\n",
      "  - Config 1/312\n",
      "    · Month 5/20 processed | running...RMSE=1.5976\n",
      "    · Month 10/20 processed | running...RMSE=1.3415\n",
      "    · Month 15/20 processed | running...RMSE=1.2784\n",
      "    · Month 20/20 processed | running...RMSE=1.1685\n",
      "  - Config 2/312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonasschernich/Documents/Masterarbeit/Code/src/models/TabPFN.py:151: UserWarning: sample_weight wird von TabPFN ignoriert.\n",
      "  warnings.warn(\"sample_weight wird von TabPFN ignoriert.\", stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    · Month 5/20 processed | running...RMSE=1.5976\n",
      "    · Month 10/20 processed | running...RMSE=1.3415\n",
      "    · Month 15/20 processed | running...RMSE=1.2784\n",
      "    · Month 20/20 processed | running...RMSE=1.1685\n",
      "  - Config 3/312\n",
      "    · Month 5/20 processed | running...RMSE=1.7509\n",
      "    · Month 10/20 processed | running...RMSE=1.5632\n",
      "    · Month 15/20 processed | running...RMSE=1.4625\n",
      "    · Month 20/20 processed | running...RMSE=1.2969\n",
      "  - Config 4/312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonasschernich/Documents/Masterarbeit/Code/src/models/TabPFN.py:151: UserWarning: sample_weight wird von TabPFN ignoriert.\n",
      "  warnings.warn(\"sample_weight wird von TabPFN ignoriert.\", stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    · Month 5/20 processed | running...RMSE=1.7509\n",
      "    · Month 10/20 processed | running...RMSE=1.5632\n",
      "    · Month 15/20 processed | running...RMSE=1.4625\n",
      "    · Month 20/20 processed | running...RMSE=1.2969\n",
      "  - Config 5/312\n",
      "    · Month 5/20 processed | running...RMSE=1.6201\n",
      "    · Month 10/20 processed | running...RMSE=1.3978\n",
      "    · Month 15/20 processed | running...RMSE=1.3814\n",
      "    · Month 20/20 processed | running...RMSE=1.2380\n",
      "  - Config 6/312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonasschernich/Documents/Masterarbeit/Code/src/models/TabPFN.py:151: UserWarning: sample_weight wird von TabPFN ignoriert.\n",
      "  warnings.warn(\"sample_weight wird von TabPFN ignoriert.\", stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    · Month 5/20 processed | running...RMSE=1.6201\n",
      "    · Month 10/20 processed | running...RMSE=1.3978\n",
      "    · Month 15/20 processed | running...RMSE=1.3814\n",
      "    · Month 20/20 processed | running...RMSE=1.2380\n",
      "  - Config 7/312\n",
      "    · Month 5/20 processed | running...RMSE=1.5973\n",
      "    · Month 10/20 processed | running...RMSE=1.3380\n",
      "    · Month 15/20 processed | running...RMSE=1.2762\n",
      "    · Month 20/20 processed | running...RMSE=1.1694\n",
      "  - Config 8/312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonasschernich/Documents/Masterarbeit/Code/src/models/TabPFN.py:151: UserWarning: sample_weight wird von TabPFN ignoriert.\n",
      "  warnings.warn(\"sample_weight wird von TabPFN ignoriert.\", stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    · Month 5/20 processed | running...RMSE=1.5973\n",
      "    · Month 10/20 processed | running...RMSE=1.3380\n",
      "    · Month 15/20 processed | running...RMSE=1.2762\n",
      "    · Month 20/20 processed | running...RMSE=1.1694\n",
      "  - Config 9/312\n",
      "    · Month 5/20 processed | running...RMSE=1.6018\n",
      "    · Month 10/20 processed | running...RMSE=1.3354\n",
      "    · Month 15/20 processed | running...RMSE=1.2768\n",
      "    · Month 20/20 processed | running...RMSE=1.1368\n",
      "  - Config 10/312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonasschernich/Documents/Masterarbeit/Code/src/models/TabPFN.py:151: UserWarning: sample_weight wird von TabPFN ignoriert.\n",
      "  warnings.warn(\"sample_weight wird von TabPFN ignoriert.\", stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    · Month 5/20 processed | running...RMSE=1.6018\n",
      "    · Month 10/20 processed | running...RMSE=1.3354\n",
      "    · Month 15/20 processed | running...RMSE=1.2768\n",
      "    · Month 20/20 processed | running...RMSE=1.1368\n",
      "  - Config 11/312\n",
      "    · Month 5/20 processed | running...RMSE=1.5415\n",
      "    · Month 10/20 processed | running...RMSE=1.2279\n",
      "    · Month 15/20 processed | running...RMSE=1.1976\n",
      "    · Month 20/20 processed | running...RMSE=1.0629\n",
      "  - Config 12/312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonasschernich/Documents/Masterarbeit/Code/src/models/TabPFN.py:151: UserWarning: sample_weight wird von TabPFN ignoriert.\n",
      "  warnings.warn(\"sample_weight wird von TabPFN ignoriert.\", stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    · Month 5/20 processed | running...RMSE=1.5415\n",
      "    · Month 10/20 processed | running...RMSE=1.2279\n",
      "    · Month 15/20 processed | running...RMSE=1.1976\n",
      "    · Month 20/20 processed | running...RMSE=1.0629\n",
      "  - Config 13/312\n",
      "    · Month 5/20 processed | running...RMSE=1.5973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "analytics-python queue is full\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    · Month 10/20 processed | running...RMSE=1.3380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "analytics-python queue is full\n",
      "analytics-python queue is full\n",
      "analytics-python queue is full\n",
      "analytics-python queue is full\n",
      "analytics-python queue is full\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    · Month 15/20 processed | running...RMSE=1.2762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "analytics-python queue is full\n",
      "analytics-python queue is full\n",
      "analytics-python queue is full\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    · Month 20/20 processed | running...RMSE=1.1694\n",
      "  - Config 14/312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonasschernich/Documents/Masterarbeit/Code/src/models/TabPFN.py:151: UserWarning: sample_weight wird von TabPFN ignoriert.\n",
      "  warnings.warn(\"sample_weight wird von TabPFN ignoriert.\", stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    · Month 5/20 processed | running...RMSE=1.5973\n",
      "    · Month 10/20 processed | running...RMSE=1.3380\n",
      "    · Month 15/20 processed | running...RMSE=1.2762\n",
      "    · Month 20/20 processed | running...RMSE=1.1694\n",
      "  - Config 15/312\n",
      "    · Month 5/20 processed | running...RMSE=1.6551\n",
      "    · Month 10/20 processed | running...RMSE=1.3408\n",
      "    · Month 15/20 processed | running...RMSE=1.2856\n",
      "    · Month 20/20 processed | running...RMSE=1.1450\n",
      "  - Config 16/312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonasschernich/Documents/Masterarbeit/Code/src/models/TabPFN.py:151: UserWarning: sample_weight wird von TabPFN ignoriert.\n",
      "  warnings.warn(\"sample_weight wird von TabPFN ignoriert.\", stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    · Month 5/20 processed | running...RMSE=1.6551\n",
      "    · Month 10/20 processed | running...RMSE=1.3408\n",
      "    · Month 15/20 processed | running...RMSE=1.2856\n",
      "    · Month 20/20 processed | running...RMSE=1.1450\n",
      "  - Config 17/312\n",
      "    · Month 5/20 processed | running...RMSE=1.7510\n",
      "    · Month 10/20 processed | running...RMSE=1.3690\n",
      "    · Month 15/20 processed | running...RMSE=1.3065\n",
      "    · Month 20/20 processed | running...RMSE=1.1925\n",
      "  - Config 18/312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonasschernich/Documents/Masterarbeit/Code/src/models/TabPFN.py:151: UserWarning: sample_weight wird von TabPFN ignoriert.\n",
      "  warnings.warn(\"sample_weight wird von TabPFN ignoriert.\", stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    · Month 5/20 processed | running...RMSE=1.7510\n",
      "    · Month 10/20 processed | running...RMSE=1.3690\n",
      "    · Month 15/20 processed | running...RMSE=1.3065\n",
      "    · Month 20/20 processed | running...RMSE=1.1925\n",
      "  - Config 19/312\n",
      "    · Month 5/20 processed | running...RMSE=1.5977\n",
      "    · Month 10/20 processed | running...RMSE=1.3305\n",
      "    · Month 15/20 processed | running...RMSE=1.2679\n",
      "    · Month 20/20 processed | running...RMSE=1.1458\n",
      "  - Config 20/312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonasschernich/Documents/Masterarbeit/Code/src/models/TabPFN.py:151: UserWarning: sample_weight wird von TabPFN ignoriert.\n",
      "  warnings.warn(\"sample_weight wird von TabPFN ignoriert.\", stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    · Month 5/20 processed | running...RMSE=1.5977\n",
      "    · Month 10/20 processed | running...RMSE=1.3305\n",
      "    · Month 15/20 processed | running...RMSE=1.2679\n",
      "    · Month 20/20 processed | running...RMSE=1.1458\n",
      "  - Config 21/312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "analytics-python queue is full\n",
      "analytics-python queue is full\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    · Month 5/20 processed | running...RMSE=2.4555\n",
      "    · Month 10/20 processed | running...RMSE=2.6512\n",
      "    · Month 15/20 processed | running...RMSE=2.3354\n",
      "    · Month 20/20 processed | running...RMSE=2.2768\n",
      "  - Config 22/312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonasschernich/Documents/Masterarbeit/Code/src/models/TabPFN.py:151: UserWarning: sample_weight wird von TabPFN ignoriert.\n",
      "  warnings.warn(\"sample_weight wird von TabPFN ignoriert.\", stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    · Month 5/20 processed | running...RMSE=2.4555\n",
      "    · Month 10/20 processed | running...RMSE=2.6512\n",
      "    · Month 15/20 processed | running...RMSE=2.3354\n",
      "    · Month 20/20 processed | running...RMSE=2.2768\n",
      "  - Config 23/312\n",
      "    · Month 5/20 processed | running...RMSE=2.5417\n",
      "    · Month 10/20 processed | running...RMSE=2.7172\n",
      "    · Month 15/20 processed | running...RMSE=2.4066\n",
      "    · Month 20/20 processed | running...RMSE=2.3521\n",
      "  - Config 24/312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonasschernich/Documents/Masterarbeit/Code/src/models/TabPFN.py:151: UserWarning: sample_weight wird von TabPFN ignoriert.\n",
      "  warnings.warn(\"sample_weight wird von TabPFN ignoriert.\", stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    · Month 5/20 processed | running...RMSE=2.5417\n",
      "    · Month 10/20 processed | running...RMSE=2.7172\n",
      "    · Month 15/20 processed | running...RMSE=2.4066\n",
      "    · Month 20/20 processed | running...RMSE=2.3521\n",
      "  - Config 25/312\n",
      "    · Month 5/20 processed | running...RMSE=1.5977\n",
      "    · Month 10/20 processed | running...RMSE=1.3305\n",
      "    · Month 15/20 processed | running...RMSE=1.2679\n",
      "    · Month 20/20 processed | running...RMSE=1.1458\n",
      "  - Config 26/312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonasschernich/Documents/Masterarbeit/Code/src/models/TabPFN.py:151: UserWarning: sample_weight wird von TabPFN ignoriert.\n",
      "  warnings.warn(\"sample_weight wird von TabPFN ignoriert.\", stacklevel=1)\n",
      "analytics-python queue is full\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    · Month 5/20 processed | running...RMSE=1.5977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "analytics-python queue is full\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    · Month 10/20 processed | running...RMSE=1.3305\n",
      "    · Month 15/20 processed | running...RMSE=1.2679\n",
      "    · Month 20/20 processed | running...RMSE=1.1458\n",
      "  - Config 27/312\n",
      "    · Month 5/20 processed | running...RMSE=2.9908\n",
      "    · Month 10/20 processed | running...RMSE=3.0032\n",
      "    · Month 15/20 processed | running...RMSE=2.7675\n",
      "    · Month 20/20 processed | running...RMSE=2.6985\n",
      "  - Config 28/312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonasschernich/Documents/Masterarbeit/Code/src/models/TabPFN.py:151: UserWarning: sample_weight wird von TabPFN ignoriert.\n",
      "  warnings.warn(\"sample_weight wird von TabPFN ignoriert.\", stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    · Month 5/20 processed | running...RMSE=2.9908\n",
      "    · Month 10/20 processed | running...RMSE=3.0032\n",
      "    · Month 15/20 processed | running...RMSE=2.7675\n",
      "    · Month 20/20 processed | running...RMSE=2.6985\n",
      "  - Config 29/312\n",
      "    · Month 5/20 processed | running...RMSE=3.0878\n",
      "    · Month 10/20 processed | running...RMSE=3.0655\n",
      "    · Month 15/20 processed | running...RMSE=2.8164\n",
      "    · Month 20/20 processed | running...RMSE=2.7424\n",
      "  - Config 30/312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonasschernich/Documents/Masterarbeit/Code/src/models/TabPFN.py:151: UserWarning: sample_weight wird von TabPFN ignoriert.\n",
      "  warnings.warn(\"sample_weight wird von TabPFN ignoriert.\", stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    · Month 5/20 processed | running...RMSE=3.0878\n",
      "    · Month 10/20 processed | running...RMSE=3.0655\n",
      "    · Month 15/20 processed | running...RMSE=2.8164\n",
      "    · Month 20/20 processed | running...RMSE=2.7424\n",
      "  - Config 31/312\n",
      "    · Month 5/20 processed | running...RMSE=1.2155\n",
      "    · Month 10/20 processed | running...RMSE=1.0351\n",
      "    · Month 15/20 processed | running...RMSE=1.0735\n",
      "    · Month 20/20 processed | running...RMSE=1.1336\n",
      "  - Config 32/312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonasschernich/Documents/Masterarbeit/Code/src/models/TabPFN.py:151: UserWarning: sample_weight wird von TabPFN ignoriert.\n",
      "  warnings.warn(\"sample_weight wird von TabPFN ignoriert.\", stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    · Month 5/20 processed | running...RMSE=1.2155\n",
      "    · Month 10/20 processed | running...RMSE=1.0351\n",
      "    · Month 15/20 processed | running...RMSE=1.0735\n",
      "    · Month 20/20 processed | running...RMSE=1.1336\n",
      "  - Config 33/312\n",
      "    · Month 5/20 processed | running...RMSE=1.7991\n",
      "    · Month 10/20 processed | running...RMSE=1.5748\n",
      "    · Month 15/20 processed | running...RMSE=1.4663\n",
      "    · Month 20/20 processed | running...RMSE=1.2923\n",
      "  - Config 34/312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonasschernich/Documents/Masterarbeit/Code/src/models/TabPFN.py:151: UserWarning: sample_weight wird von TabPFN ignoriert.\n",
      "  warnings.warn(\"sample_weight wird von TabPFN ignoriert.\", stacklevel=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    · Month 5/20 processed | running...RMSE=1.7991\n",
      "    · Month 10/20 processed | running...RMSE=1.5748\n",
      "    · Month 15/20 processed | running...RMSE=1.4663\n",
      "    · Month 20/20 processed | running...RMSE=1.2923\n",
      "  - Config 35/312\n",
      "    · Month 5/20 processed | running...RMSE=1.6026\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 263\u001B[0m\n\u001B[1;32m    252\u001B[0m         run_stageB(\n\u001B[1;32m    253\u001B[0m             model_name\u001B[38;5;241m=\u001B[39mMODEL_NAME,\n\u001B[1;32m    254\u001B[0m             model_ctor\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m hp: ForecastModel(hp),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    260\u001B[0m             rolling_imp\u001B[38;5;241m=\u001B[39mrolling_imp,\n\u001B[1;32m    261\u001B[0m         )\n\u001B[1;32m    262\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 263\u001B[0m         shortlist \u001B[38;5;241m=\u001B[39m \u001B[43mrun_stageA\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    264\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mMODEL_NAME\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    265\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmodel_ctor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mhp\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mForecastModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhp\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    266\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmodel_grid\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_grid\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    267\u001B[0m \u001B[43m            \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_ifo\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m            \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcfg\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcfg0\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m            \u001B[49m\u001B[43mkeep_top_k_final\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mmin\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodel_grid\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m            \u001B[49m\u001B[43mmin_survivors_per_block\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    273\u001B[0m         run_stageB(\n\u001B[1;32m    274\u001B[0m             model_name\u001B[38;5;241m=\u001B[39mMODEL_NAME,\n\u001B[1;32m    275\u001B[0m             model_ctor\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m hp: ForecastModel(hp),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    279\u001B[0m             cfg\u001B[38;5;241m=\u001B[39mcfg0,\n\u001B[1;32m    280\u001B[0m         )\n\u001B[1;32m    281\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/Documents/Masterarbeit/Code/src/tuning.py:568\u001B[0m, in \u001B[0;36mrun_stageA\u001B[0;34m(model_name, model_ctor, model_grid, X, y, cfg, keep_top_k_final, min_survivors_per_block, X_full_lagged, rolling_imp)\u001B[0m\n\u001B[1;32m    565\u001B[0m         X_post_ev_np \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mnan_to_num(X_post_ev\u001B[38;5;241m.\u001B[39mvalues, nan\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0\u001B[39m, posinf\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0\u001B[39m, neginf\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0\u001B[39m)\n\u001B[1;32m    566\u001B[0m         Xb_eval \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mhstack([Xb_eval, X_post_ev_np])\n\u001B[0;32m--> 568\u001B[0m y_hat \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict_one\u001B[49m\u001B[43m(\u001B[49m\u001B[43mXb_eval\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    569\u001B[0m y_true \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(y\u001B[38;5;241m.\u001B[39miloc[t \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m    571\u001B[0m y_true_block\u001B[38;5;241m.\u001B[39mappend(y_true)\n",
      "File \u001B[0;32m~/Documents/Masterarbeit/Code/src/models/TabPFN.py:172\u001B[0m, in \u001B[0;36mForecastModel.predict_one\u001B[0;34m(self, x_row)\u001B[0m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict_one\u001B[39m(\u001B[38;5;28mself\u001B[39m, x_row):\n\u001B[1;32m    171\u001B[0m     x \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(x_row)\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m--> 172\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[0;32m~/Documents/Masterarbeit/Code/src/models/TabPFN.py:167\u001B[0m, in \u001B[0;36mForecastModel.predict\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    165\u001B[0m     X_np \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(X)\n\u001B[1;32m    166\u001B[0m X_np \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_clean(X_np)\n\u001B[0;32m--> 167\u001B[0m yhat \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_np\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    168\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39masarray(yhat, dtype\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mfloat\u001B[39m)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/contextlib.py:81\u001B[0m, in \u001B[0;36mContextDecorator.__call__.<locals>.inner\u001B[0;34m(*args, **kwds)\u001B[0m\n\u001B[1;32m     78\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[1;32m     79\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minner\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds):\n\u001B[1;32m     80\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_recreate_cm():\n\u001B[0;32m---> 81\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tabpfn_common_utils/telemetry/core/decorators.py:242\u001B[0m, in \u001B[0;36mtrack_model_call.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    240\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    241\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m--> 242\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_safe_call_with_telemetry\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    243\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_method\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparam_names\u001B[49m\n\u001B[1;32m    244\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tabpfn_common_utils/telemetry/core/decorators.py:286\u001B[0m, in \u001B[0;36m_safe_call_with_telemetry\u001B[0;34m(func, args, kwargs, model_method, param_names)\u001B[0m\n\u001B[1;32m    284\u001B[0m \u001B[38;5;66;03m# Step 2: Run the actual function\u001B[39;00m\n\u001B[1;32m    285\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[0;32m--> 286\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    287\u001B[0m duration_ms \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m((time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m)\n\u001B[1;32m    289\u001B[0m \u001B[38;5;66;03m# Step 3: Send telemetry event\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tabpfn/regressor.py:870\u001B[0m, in \u001B[0;36mTabPFNRegressor.predict\u001B[0;34m(self, X, output_type, quantiles)\u001B[0m\n\u001B[1;32m    863\u001B[0m X \u001B[38;5;241m=\u001B[39m process_text_na_dataframe(X, ord_encoder\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocessor_)  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m    865\u001B[0m \u001B[38;5;66;03m# Runs over iteration engine\u001B[39;00m\n\u001B[1;32m    866\u001B[0m (\n\u001B[1;32m    867\u001B[0m     _,\n\u001B[1;32m    868\u001B[0m     outputs,  \u001B[38;5;66;03m# list of tensors [N_est, N_samples, N_borders] (after forward)\u001B[39;00m\n\u001B[1;32m    869\u001B[0m     borders,  \u001B[38;5;66;03m# list of numpy arrays containing borders for each estimator\u001B[39;00m\n\u001B[0;32m--> 870\u001B[0m ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43muse_inference_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    872\u001B[0m \u001B[38;5;66;03m# --- Translate probs, average, get final logits ---\u001B[39;00m\n\u001B[1;32m    873\u001B[0m transformed_logits \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    874\u001B[0m     translate_probs_across_borders(\n\u001B[1;32m    875\u001B[0m         logits,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    879\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m logits, borders_t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(outputs, borders)\n\u001B[1;32m    880\u001B[0m ]\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tabpfn/regressor.py:998\u001B[0m, in \u001B[0;36mTabPFNRegressor.forward\u001B[0;34m(self, X, use_inference_mode)\u001B[0m\n\u001B[1;32m    995\u001B[0m borders: \u001B[38;5;28mlist\u001B[39m[np\u001B[38;5;241m.\u001B[39mndarray] \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    997\u001B[0m \u001B[38;5;66;03m# Iterate over estimators\u001B[39;00m\n\u001B[0;32m--> 998\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecutor_\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miter_outputs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    999\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1000\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdevices\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevices_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1001\u001B[0m \u001B[43m    \u001B[49m\u001B[43mautocast\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muse_autocast_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1002\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m   1003\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msoftmax_temperature\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m!=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m:\u001B[49m\n\u001B[1;32m   1004\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43moutput\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msoftmax_temperature\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# noqa: PLW2901\u001B[39;49;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tabpfn/inference.py:507\u001B[0m, in \u001B[0;36mInferenceEngineCachePreprocessing.iter_outputs\u001B[0;34m(self, X, devices, autocast, only_return_standard_out)\u001B[0m\n\u001B[1;32m    491\u001B[0m model_forward_functions \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    492\u001B[0m     partial(\n\u001B[1;32m    493\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_model,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    503\u001B[0m     )\n\u001B[1;32m    504\u001B[0m )\n\u001B[1;32m    505\u001B[0m outputs \u001B[38;5;241m=\u001B[39m parallel_execute(devices, model_forward_functions)\n\u001B[0;32m--> 507\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mzip\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mensemble_configs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m    508\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01myield\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m_move_and_squeeze_output\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevices\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\n\u001B[1;32m    510\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minference_mode:\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tabpfn/parallel_execute.py:58\u001B[0m, in \u001B[0;36mparallel_execute\u001B[0;34m(devices, functions)\u001B[0m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Evaluate the given functions in parallel across `devices`.\u001B[39;00m\n\u001B[1;32m     40\u001B[0m \n\u001B[1;32m     41\u001B[0m \u001B[38;5;124;03mThe function evaluations are parallelised using Python threads, so this will only\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;124;03m    as `functions`.\u001B[39;00m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     56\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(devices) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;66;03m# If we only have one device then just use the current thread to avoid overhead.\u001B[39;00m\n\u001B[0;32m---> 58\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m _execute_in_current_thread(devices[\u001B[38;5;241m0\u001B[39m], functions)\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m _execute_with_multithreading(devices, functions)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tabpfn/parallel_execute.py:67\u001B[0m, in \u001B[0;36m_execute_in_current_thread\u001B[0;34m(device, functions)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_execute_in_current_thread\u001B[39m(\n\u001B[1;32m     64\u001B[0m     device: torch\u001B[38;5;241m.\u001B[39mdevice, functions: Iterable[ParallelFunction[R_co]]\n\u001B[1;32m     65\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Generator[R_co]:\n\u001B[1;32m     66\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m function \u001B[38;5;129;01min\u001B[39;00m functions:\n\u001B[0;32m---> 67\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_parallel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tabpfn/inference.py:555\u001B[0m, in \u001B[0;36mInferenceEngineCachePreprocessing._call_model\u001B[0;34m(self, device, is_parallel, X_train, X_test, y_train, cat_ix, autocast, only_return_standard_out)\u001B[0m\n\u001B[1;32m    541\u001B[0m     MemoryUsageEstimator\u001B[38;5;241m.\u001B[39mreset_peak_memory_if_required(\n\u001B[1;32m    542\u001B[0m         save_peak_mem\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msave_peak_mem,\n\u001B[1;32m    543\u001B[0m         model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    548\u001B[0m         safety_factor\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.2\u001B[39m,\n\u001B[1;32m    549\u001B[0m     )\n\u001B[1;32m    551\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m (\n\u001B[1;32m    552\u001B[0m     get_autocast_context(device, enabled\u001B[38;5;241m=\u001B[39mautocast),\n\u001B[1;32m    553\u001B[0m     torch\u001B[38;5;241m.\u001B[39minference_mode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minference_mode),\n\u001B[1;32m    554\u001B[0m ):\n\u001B[0;32m--> 555\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    556\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX_full\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    557\u001B[0m \u001B[43m        \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    558\u001B[0m \u001B[43m        \u001B[49m\u001B[43monly_return_standard_out\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43monly_return_standard_out\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    559\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcategorical_inds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatched_cat_ix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    560\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tabpfn/architectures/base/transformer.py:549\u001B[0m, in \u001B[0;36mPerFeatureTransformer.forward\u001B[0;34m(***failed resolving arguments***)\u001B[0m\n\u001B[1;32m    541\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    542\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThere should be no NaNs in the encoded x and y.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    543\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCheck that you do not feed NaNs or use a NaN-handling enocder.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    544\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYour embedded x and y returned the following:\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    545\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtorch\u001B[38;5;241m.\u001B[39misnan(embedded_x)\u001B[38;5;241m.\u001B[39many()\u001B[38;5;132;01m=}\u001B[39;00m\u001B[38;5;124m | \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtorch\u001B[38;5;241m.\u001B[39misnan(embedded_y)\u001B[38;5;241m.\u001B[39many()\u001B[38;5;132;01m=}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    546\u001B[0m     )\n\u001B[1;32m    547\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m embedded_y, embedded_x\n\u001B[0;32m--> 549\u001B[0m encoder_out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer_encoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    550\u001B[0m \u001B[43m    \u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    551\u001B[0m \u001B[43m        \u001B[49m\u001B[43membedded_input\u001B[49m\n\u001B[1;32m    552\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer_decoder\u001B[49m\n\u001B[1;32m    553\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43membedded_input\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43msingle_eval_pos\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m    554\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    555\u001B[0m \u001B[43m    \u001B[49m\u001B[43msingle_eval_pos\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msingle_eval_pos\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    556\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_trainset_representation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcache_trainset_representation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    557\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# b s f+1 e -> b s f+1 e\u001B[39;00m\n\u001B[1;32m    559\u001B[0m \u001B[38;5;66;03m# If we are using a decoder\u001B[39;00m\n\u001B[1;32m    560\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransformer_decoder:\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tabpfn/architectures/base/transformer.py:94\u001B[0m, in \u001B[0;36mLayerStack.forward\u001B[0;34m(self, x, **kwargs)\u001B[0m\n\u001B[1;32m     92\u001B[0m         x \u001B[38;5;241m=\u001B[39m checkpoint(partial(layer, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs), x, use_reentrant\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 94\u001B[0m         x \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tabpfn/architectures/base/layer.py:421\u001B[0m, in \u001B[0;36mPerFeatureEncoderLayer.forward\u001B[0;34m(self, state, single_eval_pos, cache_trainset_representation, att_src)\u001B[0m\n\u001B[1;32m    411\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    412\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPre-norm implementation is wrong, as the residual should never\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    413\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m be layer normed here.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    414\u001B[0m     )\n\u001B[1;32m    415\u001B[0m     state \u001B[38;5;241m=\u001B[39m layer_norm(\n\u001B[1;32m    416\u001B[0m         state,\n\u001B[1;32m    417\u001B[0m         allow_inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    418\u001B[0m         save_peak_mem_factor\u001B[38;5;241m=\u001B[39msave_peak_mem_factor,\n\u001B[1;32m    419\u001B[0m     )\n\u001B[0;32m--> 421\u001B[0m state \u001B[38;5;241m=\u001B[39m \u001B[43msublayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    422\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpre_norm:\n\u001B[1;32m    423\u001B[0m     state \u001B[38;5;241m=\u001B[39m layer_norm(\n\u001B[1;32m    424\u001B[0m         state,\n\u001B[1;32m    425\u001B[0m         allow_inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    426\u001B[0m         save_peak_mem_factor\u001B[38;5;241m=\u001B[39msave_peak_mem_factor,\n\u001B[1;32m    427\u001B[0m     )\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tabpfn/architectures/base/layer.py:307\u001B[0m, in \u001B[0;36mPerFeatureEncoderLayer.forward.<locals>.attn_between_features\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mattn_between_features\u001B[39m(x: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[1;32m    306\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mself_attn_between_features \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 307\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself_attn_between_features\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    308\u001B[0m \u001B[43m        \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    309\u001B[0m \u001B[43m        \u001B[49m\u001B[43msave_peak_mem_factor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msave_peak_mem_factor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    310\u001B[0m \u001B[43m        \u001B[49m\u001B[43madd_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    311\u001B[0m \u001B[43m        \u001B[49m\u001B[43mallow_inplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    312\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tabpfn/architectures/base/attention/full_attention.py:365\u001B[0m, in \u001B[0;36mMultiHeadAttention.forward\u001B[0;34m(self, x, x_kv, cache_kv, add_input, allow_inplace, save_peak_mem_factor, reuse_first_head_kv, only_cache_first_head_kv, use_cached_kv)\u001B[0m\n\u001B[1;32m    348\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_k_cache \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mempty(\n\u001B[1;32m    349\u001B[0m             batch_size,\n\u001B[1;32m    350\u001B[0m             seqlen_kv,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    354\u001B[0m             dtype\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mdtype,\n\u001B[1;32m    355\u001B[0m         )\n\u001B[1;32m    356\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_v_cache \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mempty(\n\u001B[1;32m    357\u001B[0m             batch_size,\n\u001B[1;32m    358\u001B[0m             seqlen_kv,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    362\u001B[0m             dtype\u001B[38;5;241m=\u001B[39mx\u001B[38;5;241m.\u001B[39mdtype,\n\u001B[1;32m    363\u001B[0m         )\n\u001B[0;32m--> 365\u001B[0m output: torch\u001B[38;5;241m.\u001B[39mTensor \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_compute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    366\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    367\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx_kv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    368\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_k_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    369\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_v_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    370\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_kv_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    371\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_kv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_kv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    372\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cached_kv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cached_kv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    373\u001B[0m \u001B[43m    \u001B[49m\u001B[43madd_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madd_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    374\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_inplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallow_inplace\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    375\u001B[0m \u001B[43m    \u001B[49m\u001B[43msave_peak_mem_factor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msave_peak_mem_factor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    376\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreuse_first_head_kv\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreuse_first_head_kv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    377\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    378\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m output\u001B[38;5;241m.\u001B[39mreshape(x_shape_after_transpose[:\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m+\u001B[39m output\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:])\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tabpfn/architectures/base/memory.py:95\u001B[0m, in \u001B[0;36msupport_save_peak_mem_factor.<locals>.method_\u001B[0;34m(self, x, add_input, allow_inplace, save_peak_mem_factor, *args, **kwargs)\u001B[0m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m x_, \u001B[38;5;241m*\u001B[39margs_ \u001B[38;5;129;01min\u001B[39;00m split_args:\n\u001B[1;32m     94\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m add_input:\n\u001B[0;32m---> 95\u001B[0m         x_[:] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[43mmethod\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     97\u001B[0m         x_[:] \u001B[38;5;241m=\u001B[39m method(\u001B[38;5;28mself\u001B[39m, x_, \u001B[38;5;241m*\u001B[39margs_, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tabpfn/architectures/base/attention/full_attention.py:515\u001B[0m, in \u001B[0;36mMultiHeadAttention._compute\u001B[0;34m(self, x, x_kv, k_cache, v_cache, kv_cache, cache_kv, use_cached_kv, reuse_first_head_kv)\u001B[0m\n\u001B[1;32m    502\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Attention computation.\u001B[39;00m\n\u001B[1;32m    503\u001B[0m \u001B[38;5;124;03mCalled by 'forward', potentially on shards, once shapes have been normalized.\u001B[39;00m\n\u001B[1;32m    504\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    505\u001B[0m q, k, v, kv, qkv \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_qkv(\n\u001B[1;32m    506\u001B[0m     x,\n\u001B[1;32m    507\u001B[0m     x_kv,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    513\u001B[0m     reuse_first_head_kv\u001B[38;5;241m=\u001B[39mreuse_first_head_kv,\n\u001B[1;32m    514\u001B[0m )\n\u001B[0;32m--> 515\u001B[0m attention_head_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mMultiHeadAttention\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_attention_heads\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    516\u001B[0m \u001B[43m    \u001B[49m\u001B[43mq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    517\u001B[0m \u001B[43m    \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    518\u001B[0m \u001B[43m    \u001B[49m\u001B[43mv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    519\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    520\u001B[0m \u001B[43m    \u001B[49m\u001B[43mqkv\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    521\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    522\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msoftmax_scale\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39meinsum(\n\u001B[1;32m    525\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m... h d, h d s -> ... s\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    526\u001B[0m     attention_head_outputs,\n\u001B[1;32m    527\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_w_out,\n\u001B[1;32m    528\u001B[0m )\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tabpfn/architectures/base/attention/full_attention.py:696\u001B[0m, in \u001B[0;36mMultiHeadAttention.compute_attention_heads\u001B[0;34m(q, k, v, kv, qkv, dropout_p, softmax_scale)\u001B[0m\n\u001B[1;32m    687\u001B[0m         k \u001B[38;5;241m=\u001B[39m MultiHeadAttention\u001B[38;5;241m.\u001B[39mbroadcast_kv_across_heads(\n\u001B[1;32m    688\u001B[0m             k,\n\u001B[1;32m    689\u001B[0m             share_kv_across_n_heads,\n\u001B[1;32m    690\u001B[0m         )\n\u001B[1;32m    691\u001B[0m         v \u001B[38;5;241m=\u001B[39m MultiHeadAttention\u001B[38;5;241m.\u001B[39mbroadcast_kv_across_heads(\n\u001B[1;32m    692\u001B[0m             v,\n\u001B[1;32m    693\u001B[0m             share_kv_across_n_heads,\n\u001B[1;32m    694\u001B[0m         )\n\u001B[0;32m--> 696\u001B[0m     attention_head_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunctional\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscaled_dot_product_attention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    697\u001B[0m \u001B[43m        \u001B[49m\u001B[43mq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtranspose\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    698\u001B[0m \u001B[43m        \u001B[49m\u001B[43mk\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtranspose\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    699\u001B[0m \u001B[43m        \u001B[49m\u001B[43mv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtranspose\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    700\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdropout_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdropout_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    701\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mextra_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    702\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    703\u001B[0m     attention_head_outputs \u001B[38;5;241m=\u001B[39m attention_head_outputs\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m    704\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
