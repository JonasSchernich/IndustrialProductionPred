{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-27T11:22:42.954827Z",
     "start_time": "2026-01-27T11:22:29.560173Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "# LightGBM\n",
    "# ==============================================================================\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "# --- 1) Path setup ---\n",
    "def _locate_repo_root(start: Path) -> Path:\n",
    "    \"\"\"Walk up a few levels and return the first directory that contains 'src'.\"\"\"\n",
    "    cur = start.resolve()\n",
    "    for _ in range(6):\n",
    "        if (cur / \"src\").exists():\n",
    "            return cur\n",
    "        if cur.parent == cur:\n",
    "            break\n",
    "        cur = cur.parent\n",
    "    return start.resolve()\n",
    "\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = _locate_repo_root(NOTEBOOK_DIR)\n",
    "os.environ[\"PROJECT_ROOT\"] = str(PROJECT_ROOT)\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "\n",
    "# --- 2) Imports ---\n",
    "from src.config import (\n",
    "    GlobalConfig,\n",
    "    DEFAULT_CORR_SPEC,\n",
    "    EWMA_CORR_SPEC,\n",
    "    outputs_for_model,\n",
    ")\n",
    "from src.tuning import run_stageA, run_stageB\n",
    "from src.io_timesplits import (\n",
    "    load_target,\n",
    "    load_ifo_features,\n",
    "    load_full_lagged_features,\n",
    "    load_rolling_importance,\n",
    ")\n",
    "from src.models.LGBM import ForecastModel  # LightGBM model wrapper\n",
    "\n",
    "\n",
    "# --- 3) Config ---\n",
    "USE_DYNAMIC_FI_PIPELINE = False  # False = standard setup (I & II)\n",
    "SEED = 42\n",
    "N_JOBS = 1  # LightGBM parallelizes internally; grid search often works best with 1\n",
    "\n",
    "MODEL_NAME = \"lightgbm_dynamic_fi_dummy\" if USE_DYNAMIC_FI_PIPELINE else \"lightgbm_dummy\"\n",
    "\n",
    "outputs_for_model(MODEL_NAME)\n",
    "print(f\"--- Starting tuning for: {MODEL_NAME} ---\")\n",
    "\n",
    "\n",
    "# --- 4) Load data ---\n",
    "y = load_target()\n",
    "X_ifo = load_ifo_features()\n",
    "\n",
    "# Align indices\n",
    "idx_common = y.index.intersection(X_ifo.index)\n",
    "y = y.loc[idx_common]\n",
    "X_ifo = X_ifo.loc[idx_common]\n",
    "\n",
    "X_full_lagged = None\n",
    "rolling_imp = None\n",
    "y_fi = None\n",
    "\n",
    "if USE_DYNAMIC_FI_PIPELINE:\n",
    "    FI_BASE_DIR = PROJECT_ROOT / \"outputs\" / \"feature_importance\" / \"outputs_no_missing\"\n",
    "    try:\n",
    "        X_full_lagged = load_full_lagged_features(base_dir=FI_BASE_DIR)\n",
    "        rolling_imp = load_rolling_importance(base_dir=FI_BASE_DIR)\n",
    "\n",
    "        idx_fi = y.index.intersection(X_full_lagged.index).intersection(rolling_imp.index)\n",
    "        y_fi = y.loc[idx_fi]\n",
    "        X_full_lagged = X_full_lagged.loc[idx_fi]\n",
    "        rolling_imp = rolling_imp.loc[idx_fi]\n",
    "        print(f\"Dynamic FI mode: loaded {X_full_lagged.shape[1]} features.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"ERROR: Dynamic FI artifacts not found.\")\n",
    "        sys.exit(1)\n",
    "else:\n",
    "    print(f\"Full FE mode (Setup I/II): {X_ifo.shape[1]} base features.\")\n",
    "\n",
    "\n",
    "# --- 5) Config defaults (thesis policy) ---\n",
    "def get_thesis_cfg() -> GlobalConfig:\n",
    "    cfg = GlobalConfig(preset=\"thesis\")\n",
    "    cfg.policy_window = 24\n",
    "    cfg.policy_decay = 0.97\n",
    "    cfg.selection_mode = \"decayed_best\"\n",
    "    return cfg\n",
    "\n",
    "\n",
    "cfg_obj = get_thesis_cfg()\n",
    "\n",
    "\n",
    "# --- 6) Grid definition ---\n",
    "def build_grid_full_fe():\n",
    "    \"\"\"Setup I (ifo) and Setup II (ifo + target blocks).\"\"\"\n",
    "\n",
    "    # A) Feature engineering & dimensionality reduction\n",
    "    lag_candidates = [tuple(range(7))]\n",
    "\n",
    "    corr_opts = [\n",
    "        {\"corr_spec\": dict(DEFAULT_CORR_SPEC)},\n",
    "        {\"corr_spec\": dict(EWMA_CORR_SPEC)},\n",
    "    ]\n",
    "\n",
    "    k1_opts = [50]\n",
    "    red_opts = [1.0]\n",
    "\n",
    "    dr_opts = [\n",
    "        {\"dr_method\": \"none\"},\n",
    "        # {\"dr_method\": \"pca\", \"pca_kmax\": 30, \"pca_var_target\": 0.99},\n",
    "        # {\"dr_method\": \"pls\", \"pls_components\": 30},\n",
    "    ]\n",
    "\n",
    "    # B) Target blocks (Setup II)\n",
    "    block_opts = [\n",
    "        # None,  # Setup I\n",
    "        [\"AR1\", \"Chronos\", \"TSFresh\"],  # Setup IIb\n",
    "    ]\n",
    "\n",
    "    # C) Sample weights\n",
    "    weight_opts = [\n",
    "        {\"sample_weight_decay\": None},\n",
    "        {\"sample_weight_decay\": 0.99},\n",
    "    ]\n",
    "\n",
    "    # D) LightGBM hyperparameters\n",
    "    n_estimators_list = [5000]\n",
    "    early_stopping_rounds_list = [50]\n",
    "    val_tail_list = [24]\n",
    "\n",
    "    learning_rate_list = [0.02, 0.05]\n",
    "    num_leaves_list = [31, 63, 127]\n",
    "    max_depth_list = [4, 7, -1]\n",
    "    min_child_samples_list = [50, 150]\n",
    "    colsample_bytree_list = [0.8]\n",
    "    reg_lambda_list = [0, 1, 10]\n",
    "\n",
    "    grid = []\n",
    "\n",
    "    # 1) FE loop\n",
    "    for lags, corr, k1, red, dr in product(lag_candidates, corr_opts, k1_opts, red_opts, dr_opts):\n",
    "        base_fe = {\n",
    "            \"lag_candidates\": lags,\n",
    "            \"k1_topk\": k1,\n",
    "            \"redundancy_param\": red,\n",
    "            **dr,\n",
    "            **corr,\n",
    "        }\n",
    "\n",
    "        # 2) Blocks & weights\n",
    "        for blocks, weights in product(block_opts, weight_opts):\n",
    "            # 3) Model HPs\n",
    "            for lr, leaves, depth, min_child, colsample, lam, est, esr, vt in product(\n",
    "                learning_rate_list,\n",
    "                num_leaves_list,\n",
    "                max_depth_list,\n",
    "                min_child_samples_list,\n",
    "                colsample_bytree_list,\n",
    "                reg_lambda_list,\n",
    "                n_estimators_list,\n",
    "                early_stopping_rounds_list,\n",
    "                val_tail_list,\n",
    "            ):\n",
    "                hp = {\n",
    "                    **base_fe,\n",
    "                    \"target_block_set\": blocks,\n",
    "                    **weights,\n",
    "                    \"n_estimators\": est,\n",
    "                    \"learning_rate\": lr,\n",
    "                    \"num_leaves\": leaves,\n",
    "                    \"max_depth\": depth,\n",
    "                    \"min_child_samples\": min_child,\n",
    "                    \"colsample_bytree\": colsample,\n",
    "                    \"reg_lambda\": lam,\n",
    "                    # Early stopping\n",
    "                    \"early_stopping_rounds\": esr,\n",
    "                    \"val_tail\": vt,\n",
    "                    # Fixed params\n",
    "                    \"min_child_weight\": 1e-3,\n",
    "                    \"importance_type\": \"gain\",\n",
    "                    \"n_jobs\": N_JOBS,\n",
    "                    \"seed\": SEED,\n",
    "                }\n",
    "                grid.append(hp)\n",
    "\n",
    "    return grid\n",
    "\n",
    "\n",
    "def build_grid_dynamic_fi():\n",
    "    \"\"\"Setup III: Dynamic feature importance via strict Top-N selection.\"\"\"\n",
    "\n",
    "    n_features_list = [50]  # number of features to keep\n",
    "    weight_opts = [{\"sample_weight_decay\": None}, {\"sample_weight_decay\": 0.99}]\n",
    "\n",
    "    # LightGBM hyperparameters\n",
    "    n_estimators_list = [5000]\n",
    "    early_stopping_rounds_list = [50]\n",
    "    val_tail_list = [24]\n",
    "\n",
    "    learning_rate_list = [0.02, 0.05]\n",
    "    num_leaves_list = [31, 63, 127]\n",
    "    max_depth_list = [4, 7, -1]\n",
    "    min_child_samples_list = [50, 150]\n",
    "    colsample_bytree_list = [0.6, 0.9]\n",
    "    reg_lambda_list = [1, 10]\n",
    "\n",
    "    grid = []\n",
    "    for n_feat, w in product(n_features_list, weight_opts):\n",
    "        for lr, leaves, depth, min_child, colsample, lam, est, esr, vt in product(\n",
    "            learning_rate_list,\n",
    "            num_leaves_list,\n",
    "            max_depth_list,\n",
    "            min_child_samples_list,\n",
    "            colsample_bytree_list,\n",
    "            reg_lambda_list,\n",
    "            n_estimators_list,\n",
    "            early_stopping_rounds_list,\n",
    "            val_tail_list,\n",
    "        ):\n",
    "            hp = {\n",
    "                \"n_features_to_use\": n_feat,\n",
    "                **w,\n",
    "                \"n_estimators\": est,\n",
    "                \"learning_rate\": lr,\n",
    "                \"num_leaves\": leaves,\n",
    "                \"max_depth\": depth,\n",
    "                \"min_child_samples\": min_child,\n",
    "                \"colsample_bytree\": colsample,\n",
    "                \"reg_lambda\": lam,\n",
    "                \"early_stopping_rounds\": esr,\n",
    "                \"val_tail\": vt,\n",
    "                \"min_child_weight\": 1e-3,\n",
    "                \"importance_type\": \"gain\",\n",
    "                \"n_jobs\": N_JOBS,\n",
    "                \"seed\": SEED,\n",
    "            }\n",
    "            grid.append(hp)\n",
    "    return grid\n",
    "\n",
    "\n",
    "# --- 7) Run ---\n",
    "if USE_DYNAMIC_FI_PIPELINE:\n",
    "    grid = build_grid_dynamic_fi()\n",
    "    print(f\"Dynamic FI grid size (Setup III): {len(grid)} configurations.\")\n",
    "\n",
    "    shortlist = run_stageA(\n",
    "        model_name=MODEL_NAME,\n",
    "        model_ctor=lambda hp: ForecastModel(hp),\n",
    "        model_grid=grid,\n",
    "        X=X_ifo,  # dummy\n",
    "        y=y_fi,\n",
    "        cfg=cfg_obj,\n",
    "        X_full_lagged=X_full_lagged,\n",
    "        rolling_imp=rolling_imp,\n",
    "        keep_top_k_final=5,\n",
    "        min_survivors_per_block=5,\n",
    "    )\n",
    "\n",
    "    run_stageB(\n",
    "        model_name=MODEL_NAME,\n",
    "        model_ctor=lambda hp: ForecastModel(hp),\n",
    "        shortlist=shortlist,\n",
    "        X=X_ifo,  # dummy\n",
    "        y=y_fi,\n",
    "        cfg=cfg_obj,\n",
    "        X_full_lagged=X_full_lagged,\n",
    "        rolling_imp=rolling_imp,\n",
    "    )\n",
    "\n",
    "else:\n",
    "    grid = build_grid_full_fe()\n",
    "    print(f\"Full FE grid size (Setup I & II): {len(grid)} configurations.\")\n",
    "\n",
    "    # Note: this grid is large; Stage A filtering is important.\n",
    "    shortlist = run_stageA(\n",
    "        model_name=MODEL_NAME,\n",
    "        model_ctor=lambda hp: ForecastModel(hp),\n",
    "        model_grid=grid,\n",
    "        X=X_ifo,\n",
    "        y=y,\n",
    "        cfg=cfg_obj,\n",
    "        keep_top_k_final=5,\n",
    "        min_survivors_per_block=5,\n",
    "    )\n",
    "\n",
    "    run_stageB(\n",
    "        model_name=MODEL_NAME,\n",
    "        model_ctor=lambda hp: ForecastModel(hp),\n",
    "        shortlist=shortlist,\n",
    "        X=X_ifo,\n",
    "        y=y,\n",
    "        cfg=cfg_obj,\n",
    "    )\n",
    "\n",
    "print(\"\\nTuning complete.\")\n"
   ],
   "id": "13df4a1065bb4f3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting tuning for: lightgbm_dummy ---\n",
      "INFO load_ifo_features: Renaming columns for validity.\n",
      "Full FE mode (Setup I/II): 2160 base features.\n",
      "Full FE grid size (Setup I & II): 432 configurations.\n",
      "[Stage A] Using FULL FE (tracks 1 & 2) pipeline.\n",
      "[Stage A][Block 1] train_end=180, OOS=181-200 | configs=432\n",
      "  - Config 1/432\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[148]\tvalid_0's l2: 1.28355\n",
      "    · Month 5/20 processed | running RMSE=1.6748\n",
      "    · Month 10/20 processed | running RMSE=1.3337\n",
      "    · Month 15/20 processed | running RMSE=1.2366\n",
      "    · Month 20/20 processed | running RMSE=1.1036\n",
      "  - Config 2/432\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[171]\tvalid_0's l2: 1.28164\n",
      "    · Month 5/20 processed | running RMSE=1.6855\n",
      "    · Month 10/20 processed | running RMSE=1.3425\n",
      "    · Month 15/20 processed | running RMSE=1.2433\n",
      "    · Month 20/20 processed | running RMSE=1.1101\n",
      "  - Config 3/432\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[187]\tvalid_0's l2: 1.2813\n",
      "    · Month 5/20 processed | running RMSE=1.6815\n",
      "    · Month 10/20 processed | running RMSE=1.3384\n",
      "    · Month 15/20 processed | running RMSE=1.2393\n",
      "    · Month 20/20 processed | running RMSE=1.1051\n",
      "  - Config 4/432\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1]\tvalid_0's l2: 1.61302\n",
      "    · Month 5/20 processed | running RMSE=1.6554\n",
      "    · Month 10/20 processed | running RMSE=1.3242\n",
      "    · Month 15/20 processed | running RMSE=1.2425\n",
      "    · Month 20/20 processed | running RMSE=1.1104\n",
      "  - Config 5/432\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 284\u001B[0m\n\u001B[1;32m    281\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFull FE grid size (Setup I & II): \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(grid)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m configurations.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    283\u001B[0m     \u001B[38;5;66;03m# Note: this grid is large; Stage A filtering is important.\u001B[39;00m\n\u001B[0;32m--> 284\u001B[0m     shortlist \u001B[38;5;241m=\u001B[39m \u001B[43mrun_stageA\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    285\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mMODEL_NAME\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    286\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel_ctor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mhp\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mForecastModel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhp\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    287\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel_grid\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrid\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    288\u001B[0m \u001B[43m        \u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mX_ifo\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    289\u001B[0m \u001B[43m        \u001B[49m\u001B[43my\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    290\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcfg\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcfg_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    291\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkeep_top_k_final\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    292\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmin_survivors_per_block\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    293\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    295\u001B[0m     run_stageB(\n\u001B[1;32m    296\u001B[0m         model_name\u001B[38;5;241m=\u001B[39mMODEL_NAME,\n\u001B[1;32m    297\u001B[0m         model_ctor\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mlambda\u001B[39;00m hp: ForecastModel(hp),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    301\u001B[0m         cfg\u001B[38;5;241m=\u001B[39mcfg_obj,\n\u001B[1;32m    302\u001B[0m     )\n\u001B[1;32m    304\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mTuning complete.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/Masterarbeit/Code/src/tuning.py:483\u001B[0m, in \u001B[0;36mrun_stageA\u001B[0;34m(model_name, model_ctor, model_grid, X, y, cfg, keep_top_k_final, min_survivors_per_block, X_full_lagged, rolling_imp)\u001B[0m\n\u001B[1;32m    478\u001B[0m kept \u001B[38;5;241m=\u001B[39m redundancy_reduce_greedy(\n\u001B[1;32m    479\u001B[0m     X_sel, hp_eff[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcorr_spec\u001B[39m\u001B[38;5;124m\"\u001B[39m], taus_scr, hp_eff[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mredundancy_param\u001B[39m\u001B[38;5;124m\"\u001B[39m], scores\u001B[38;5;241m=\u001B[39mscores\n\u001B[1;32m    480\u001B[0m )\n\u001B[1;32m    482\u001B[0m \u001B[38;5;66;03m# 5) OOS matrix\u001B[39;00m\n\u001B[0;32m--> 483\u001B[0m X_eng_full \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_engineered_matrix\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlag_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    485\u001B[0m X_aug_full_pre_dr, _ \u001B[38;5;241m=\u001B[39m _augment_with_target_blocks(X_eng_full, pre_dr_blocks)\n\u001B[1;32m    486\u001B[0m X_red_pre_dr \u001B[38;5;241m=\u001B[39m X_aug_full_pre_dr\u001B[38;5;241m.\u001B[39mloc[:, kept]\n",
      "File \u001B[0;32m~/Documents/Masterarbeit/Code/src/features.py:118\u001B[0m, in \u001B[0;36mbuild_engineered_matrix\u001B[0;34m(X, lag_map)\u001B[0m\n\u001B[1;32m    116\u001B[0m     s \u001B[38;5;241m=\u001B[39m X[col]\n\u001B[1;32m    117\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m lag \u001B[38;5;129;01min\u001B[39;00m lags:\n\u001B[0;32m--> 118\u001B[0m         out[\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcol\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m__lag\u001B[39m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mlag\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m s\u001B[38;5;241m.\u001B[39mshift(lag)\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mfloat\u001B[39m)\n\u001B[1;32m    119\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m pd\u001B[38;5;241m.\u001B[39mDataFrame(out, index\u001B[38;5;241m=\u001B[39mX\u001B[38;5;241m.\u001B[39mindex)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
