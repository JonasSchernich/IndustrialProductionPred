{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- Setup & Imports ---\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product  # NEU: für flachere Grid-Erzeugung\n",
    "\n",
    "# --- 1. Pfad-Setup (wie in ET.ipynb) ---\n",
    "def _locate_repo_root(start: Path) -> Path:\n",
    "    cur = start.resolve()\n",
    "    for _ in range(5):\n",
    "        if (cur / 'src').exists():\n",
    "            return cur\n",
    "        if cur.parent == cur:\n",
    "            break\n",
    "        cur = cur.parent\n",
    "    return start.resolve()\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = _locate_repo_root(NOTEBOOK_DIR)\n",
    "os.environ['PROJECT_ROOT'] = str(PROJECT_ROOT)\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.config import GlobalConfig, DEFAULT_CORR_SPEC, EWMA_CORR_SPEC, outputs_for_model\n",
    "from src.tuning import run_stageA, run_stageB\n",
    "from src.io_timesplits import (\n",
    "    load_target, load_ifo_features,\n",
    "    load_full_lagged_features, load_rolling_importance\n",
    ")\n",
    "# --- MODELL-IMPORT (LGBM-Wrapper analog ET/EN API) ---\n",
    "from src.models.LGBM import ForecastModel  # nutzt LightGBM Regressor-Wrapper\n",
    "\n",
    "print('PROJECT_ROOT =', PROJECT_ROOT)\n",
    "\n",
    "# --- Pfad zur Feature-Importance-Outputs (wie ET) ---\n",
    "FI_PATH = PROJECT_ROOT / \"outputs\" / \"feature_importance\" / \"outputs_no_missing\"\n",
    "\n",
    "# --- MASTER-SCHALTER ---\n",
    "# False => \"Full FE\" (normale FE-Pipeline mit ifo + optionalen Target-Blöcken)\n",
    "# True  => \"Dynamic FI\" (rolling Feature Importance Top-N)\n",
    "USE_DYNAMIC_FI_PIPELINE = False\n",
    "\n",
    "# --- MODELLNAME ---\n",
    "if USE_DYNAMIC_FI_PIPELINE:\n",
    "    MODEL_NAME = \"lightgbm_dynamic_fi\"  # separater Output-Ordner\n",
    "else:\n",
    "    MODEL_NAME = \"lightgbm\"             # originaler Ordner\n",
    "\n",
    "# Optional: Ressourcen/Seeds\n",
    "SEED   = 42\n",
    "N_JOBS = 1\n",
    "\n",
    "outputs_for_model(MODEL_NAME)\n",
    "print(f'Modell {MODEL_NAME} wird getunt.')\n",
    "\n",
    "# --- 2. Daten laden (wie ET) ---\n",
    "y = load_target()             # ΔIP (DatetimeIndex)\n",
    "X_ifo = load_ifo_features()   # ifo features (für Full FE)\n",
    "\n",
    "if USE_DYNAMIC_FI_PIPELINE:\n",
    "    # Dynamic-FI Artefakte laden\n",
    "    try:\n",
    "        X_full_lagged = load_full_lagged_features(base_dir=FI_PATH)\n",
    "        rolling_imp   = load_rolling_importance(base_dir=FI_PATH)\n",
    "        idx_fi = y.index.intersection(X_full_lagged.index).intersection(rolling_imp.index)\n",
    "        y_fi, X_full_lagged, rolling_imp = y.loc[idx_fi], X_full_lagged.loc[idx_fi], rolling_imp.loc[idx_fi]\n",
    "        print('Dynamic-FI Daten geladen. Shapes:', X_full_lagged.shape, rolling_imp.shape)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"FEHLER: {e}\")\n",
    "        print(\"Stelle sicher, dass feature_importance.ipynb (entspr. Jobs) vorher gelaufen ist.\")\n",
    "        raise\n",
    "else:\n",
    "    X_full_lagged, rolling_imp = None, None\n",
    "    idx = y.index.intersection(X_ifo.index)\n",
    "    y, X_ifo = y.loc[idx], X_ifo.loc[idx]\n",
    "    print('Full-FE Daten geladen. Shapes:', X_ifo.shape, y.shape)\n",
    "\n",
    "# --- 3. Base config (Splits & Policy wie in der Thesis) ---\n",
    "def base_cfg() -> GlobalConfig:\n",
    "    cfg = GlobalConfig(preset=\"thesis\")  # lädt Thesis-Splits (180, 240, ...)\n",
    "    cfg.policy_window   = 24\n",
    "    cfg.policy_decay    = 0.95\n",
    "    cfg.policy_gain_min = 0.03\n",
    "    cfg.policy_cooldown = 3\n",
    "    return cfg\n",
    "\n",
    "cfg0 = base_cfg()\n",
    "\n",
    "# --- 4. Helper für Korrelations-Spezifikation (gleich wie ET) ---\n",
    "def make_corr_spec(kind: str) -> dict:\n",
    "    if kind == 'expanding':\n",
    "        return dict(DEFAULT_CORR_SPEC)\n",
    "    elif kind == 'ewm':\n",
    "        return dict(EWMA_CORR_SPEC)\n",
    "    else:\n",
    "        raise ValueError(\"kind must be 'expanding' oder 'ewm'\")\n",
    "\n",
    "# --- 5. TUNING-GRID (Logik-Switch) ---\n",
    "if USE_DYNAMIC_FI_PIPELINE:\n",
    "    # --- GRID FÜR Dynamic FI (Nur Modell-HPs + n_features_to_use) ---\n",
    "    print(\"Erstelle HP-Grid für 'Dynamic FI' ...\")\n",
    "\n",
    "    N_FEATURES_TO_USE = 20  # Top-N Features aus rolling FI\n",
    "\n",
    "    # LGBM-HP-Listen (kompakt)\n",
    "    n_estimators_list       = [1200]\n",
    "    learning_rate_list      = [0.02, 0.05]\n",
    "    num_leaves_list         = [64, 128]\n",
    "    max_depth_list          = [-1, 6]\n",
    "    min_child_samples_list  = [20, 50]\n",
    "    colsample_bytree_list   = [0.8, 1.0]\n",
    "    subsample_list          = [0.8, 1.0]\n",
    "    bagging_freq_list       = [0]\n",
    "    max_bin_list            = [255]\n",
    "    reg_alpha_list          = [0, 1]\n",
    "    reg_lambda_list         = [0, 1]\n",
    "    min_split_gain_list     = [0.0]\n",
    "    min_child_weight_list   = [1e-3, 1.0]\n",
    "    early_stopping_rounds_list = [None, 100]  # val_tail muss dann gesetzt werden\n",
    "    val_tail_list           = [None, 24]\n",
    "    weighting_options = [\n",
    "        {\"sample_weight_decay\": None},\n",
    "        {\"sample_weight_decay\": 0.98},\n",
    "    ]\n",
    "\n",
    "    def build_model_grid_dynamic_fi():\n",
    "        hp_names = [\n",
    "            'n_estimators','learning_rate','num_leaves','max_depth','min_child_samples',\n",
    "            'colsample_bytree','subsample','bagging_freq','max_bin','reg_alpha',\n",
    "            'reg_lambda','min_split_gain','min_child_weight',\n",
    "            'early_stopping_rounds','val_tail'\n",
    "        ]\n",
    "        hp_lists = [\n",
    "            n_estimators_list, learning_rate_list, num_leaves_list, max_depth_list, min_child_samples_list,\n",
    "            colsample_bytree_list, subsample_list, bagging_freq_list, max_bin_list, reg_alpha_list,\n",
    "            reg_lambda_list, min_split_gain_list, min_child_weight_list,\n",
    "            early_stopping_rounds_list, val_tail_list\n",
    "        ]\n",
    "\n",
    "        grid = []\n",
    "        for vals in product(*hp_lists):\n",
    "            base = dict(zip(hp_names, vals))\n",
    "            base.update({\n",
    "                'n_features_to_use': N_FEATURES_TO_USE,\n",
    "                'importance_type': 'gain',\n",
    "                'seed': SEED,\n",
    "                'n_jobs': N_JOBS,\n",
    "            })\n",
    "            for w in weighting_options:\n",
    "                hp = dict(base)\n",
    "                hp.update(w)\n",
    "                grid.append(hp)\n",
    "        return grid\n",
    "\n",
    "    model_grid = build_model_grid_dynamic_fi()\n",
    "\n",
    "else:\n",
    "    # --- GRID FÜR Full FE (ET-Logik, aber LGBM-HPs) ---\n",
    "    print(\"Erstelle HP-Grid für 'Full FE' ...\")\n",
    "\n",
    "    # A) Feature Engineering Parameter (reduziert wie in ET.ipynb)\n",
    "    corr_options = [\n",
    "        (\"expanding\", make_corr_spec(\"expanding\")),\n",
    "        (\"ewm\",       make_corr_spec(\"ewm\")),\n",
    "    ]\n",
    "    lag_candidates_list = [(1, 2, 3, 6, 12)]\n",
    "    top_k_lags_list     = [1]      # reduziert\n",
    "    use_rm3_list        = [True]   # reduziert\n",
    "    k1_topk_list        = [100, 300]\n",
    "    redundancy_param_list = [0.90] # reduziert\n",
    "    dr_options_list     = [\n",
    "        {'dr_method': 'none'},\n",
    "        {'dr_method': 'pca', 'pca_var_target': 0.95, 'pca_kmax': 50},\n",
    "        {'dr_method': 'pls', 'pls_components': 8},\n",
    "    ]\n",
    "\n",
    "    # B) LGBM Hyperparameter (schlank, ET-Stil)\n",
    "    n_estimators_list       = [600, 1200]\n",
    "    learning_rate_list      = [0.02, 0.05]\n",
    "    num_leaves_list         = [31, 64]\n",
    "    max_depth_list          = [-1, 6]\n",
    "    min_child_samples_list  = [20, 50]\n",
    "    colsample_bytree_list   = [0.8, 1.0]\n",
    "    subsample_list          = [0.8, 1.0]\n",
    "    bagging_freq_list       = [0]\n",
    "    max_bin_list            = [255]\n",
    "    reg_alpha_list          = [0, 1]\n",
    "    reg_lambda_list         = [0, 1]\n",
    "    min_split_gain_list     = [0.0]\n",
    "    min_child_weight_list   = [1e-3, 1.0]\n",
    "    early_stopping_rounds_list = [None, 100]  # bei 100 wird val_tail=24 sinnvoll\n",
    "    val_tail_list           = [None, 24]\n",
    "\n",
    "    # C) Target Blocks & Weighting\n",
    "    target_block_options = [None, [\"AR1\"], [\"Chronos\"], [\"TSFresh\"]]\n",
    "    weighting_options    = [{\"sample_weight_decay\": None}]  # reduziert\n",
    "\n",
    "    # D) Grid zusammensetzen\n",
    "    def build_model_grid_full_fe():\n",
    "        hp_grid = []\n",
    "\n",
    "        # FE/DR-Listen in ein Produkt packen\n",
    "        fe_lists = [\n",
    "            lag_candidates_list,      # lags\n",
    "            top_k_lags_list,          # k_lags\n",
    "            use_rm3_list,             # rm3\n",
    "            k1_topk_list,             # k1\n",
    "            redundancy_param_list,    # red\n",
    "            dr_options_list,          # dr_opt (dict)\n",
    "        ]\n",
    "\n",
    "        # LGBM-HP-Produkt\n",
    "        lgbm_names = [\n",
    "            'n_estimators','learning_rate','num_leaves','max_depth','min_child_samples',\n",
    "            'colsample_bytree','subsample','bagging_freq','max_bin','reg_alpha',\n",
    "            'reg_lambda','min_split_gain','min_child_weight',\n",
    "            'early_stopping_rounds','val_tail'\n",
    "        ]\n",
    "        lgbm_lists = [\n",
    "            n_estimators_list, learning_rate_list, num_leaves_list, max_depth_list, min_child_samples_list,\n",
    "            colsample_bytree_list, subsample_list, bagging_freq_list, max_bin_list, reg_alpha_list,\n",
    "            reg_lambda_list, min_split_gain_list, min_child_weight_list,\n",
    "            early_stopping_rounds_list, val_tail_list\n",
    "        ]\n",
    "\n",
    "        for (corr_tag, corr_spec) in corr_options:\n",
    "            for (lags, k_lags, rm3, k1, red, dr_opt) in product(*fe_lists):\n",
    "                # Einschränkung wie im Original:\n",
    "                if k1 == 100 and dr_opt['dr_method'] != 'none':\n",
    "                    continue\n",
    "\n",
    "                # alle LGBM-Kombis\n",
    "                for lvals in product(*lgbm_lists):\n",
    "                    lgbm_hp = dict(zip(lgbm_names, lvals))\n",
    "\n",
    "                    base = {\n",
    "                        # FE/DR\n",
    "                        'lag_candidates': lags,\n",
    "                        'top_k_lags_per_feature': k_lags,\n",
    "                        'use_rm3': rm3,\n",
    "                        'k1_topk': k1,\n",
    "                        'redundancy_param': red,\n",
    "                        **dr_opt,\n",
    "                        'corr_tag': corr_tag,\n",
    "                        'corr_spec': corr_spec,\n",
    "                    }\n",
    "\n",
    "                    for block_set in target_block_options:\n",
    "                        for w in weighting_options:\n",
    "                            hp = {\n",
    "                                **base,\n",
    "                                **lgbm_hp,\n",
    "                                'target_block_set': block_set,\n",
    "                                'importance_type': 'gain',\n",
    "                                'seed': SEED,\n",
    "                                'n_jobs': N_JOBS,\n",
    "                                **w,\n",
    "                            }\n",
    "                            hp_grid.append(hp)\n",
    "\n",
    "        return hp_grid\n",
    "\n",
    "    model_grid = build_model_grid_full_fe()\n",
    "\n",
    "print(\"Optimierte HP-Kombinationen:\", len(model_grid))\n",
    "print(\"Erstes HP-Set:\", model_grid[0] if model_grid else \"Grid ist leer\")\n",
    "\n",
    "# --- 6. Stage A/B Lauf (Logik-Switch wie ET) ---\n",
    "if model_grid:\n",
    "    if USE_DYNAMIC_FI_PIPELINE:\n",
    "        # --- Dynamic FI Lauf ---\n",
    "        shortlist = run_stageA(\n",
    "            model_name=MODEL_NAME,\n",
    "            model_ctor=lambda hp: ForecastModel(hp),\n",
    "            model_grid=model_grid,\n",
    "            X=X_ifo,  # Platzhalter (nicht genutzt in Dynamic-FI)\n",
    "            y=y_fi,\n",
    "            cfg=cfg0,\n",
    "            keep_top_k_final=min(5, len(model_grid)),\n",
    "            min_survivors_per_block=max(1, len(model_grid)//4),\n",
    "            # Dynamic-FI Inputs:\n",
    "            X_full_lagged=X_full_lagged,\n",
    "            rolling_imp=rolling_imp,\n",
    "        )\n",
    "\n",
    "        run_stageB(\n",
    "            model_name=MODEL_NAME,\n",
    "            model_ctor=lambda hp: ForecastModel(hp),\n",
    "            shortlist=shortlist,\n",
    "            X=X_ifo,  # Platzhalter\n",
    "            y=y_fi,\n",
    "            cfg=cfg0,\n",
    "            X_full_lagged=X_full_lagged,\n",
    "            rolling_imp=rolling_imp,\n",
    "        )\n",
    "    else:\n",
    "        # --- Full FE Lauf ---\n",
    "        shortlist = run_stageA(\n",
    "            model_name=MODEL_NAME,\n",
    "            model_ctor=lambda hp: ForecastModel(hp),\n",
    "            model_grid=model_grid,\n",
    "            X=X_ifo,\n",
    "            y=y,\n",
    "            cfg=cfg0,\n",
    "            keep_top_k_final=min(5, len(model_grid)),\n",
    "            min_survivors_per_block=max(1, len(model_grid)//4),\n",
    "        )\n",
    "\n",
    "        run_stageB(\n",
    "            model_name=MODEL_NAME,\n",
    "            model_ctor=lambda hp: ForecastModel(hp),\n",
    "            shortlist=shortlist,\n",
    "            X=X_ifo,\n",
    "            y=y,\n",
    "            cfg=cfg0,\n",
    "        )\n",
    "else:\n",
    "    print(\"Keine gültigen HP-Kombinationen gefunden, Stages übersprungen.\")\n",
    "\n",
    "print(f\"\\nDone. Check outputs/stageA|stageB/{MODEL_NAME} for results.\")\n",
    "\n"
   ],
   "id": "13df4a1065bb4f3c"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
