{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71b2b7cf",
   "metadata": {},
   "source": [
    "# TSFresh Slim Precompute (Windowed, Causal)\n",
    "Erzeugt eine **schlanke**, kausal korrekte tsfresh-Featurematrix als Parquet.\n",
    "- nutzt `EfficientFCParameters`\n",
    "- fensterbasiert via `roll_time_series`\n",
    "- **kein** shift hier; shift(1) passiert beim Merge."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-08T15:36:25.601630Z",
     "start_time": "2025-11-08T15:36:06.810004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os, sys  # Hinzugefügt\n",
    "\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction.settings import EfficientFCParameters\n",
    "from tsfresh.utilities.dataframe_functions import roll_time_series\n",
    "\n",
    "# --- 1. Robuster Pfad-Setup ---\n",
    "def _locate_repo_root(start: Path) -> Path:\n",
    "    \"\"\"Findet das Projektverzeichnis, indem es nach 'src' oder 'data' sucht.\"\"\"\n",
    "    cur = start.resolve()\n",
    "    for _ in range(5):  # Bis zu 5 Ebenen nach oben suchen\n",
    "        if (cur / 'src').exists() or (cur / 'data').exists():\n",
    "            return cur\n",
    "        if cur.parent == cur:\n",
    "            break\n",
    "        cur = cur.parent\n",
    "    # Fallback: Nimm an, wir sind 2 Ebenen tief (Standard-Notebook-Ordner)\n",
    "    print(\"Warnung: Konnte 'src' oder 'data' Ordner nicht finden. Rate Projekt-Root.\")\n",
    "    return start.resolve().parent.parent\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = _locate_repo_root(NOTEBOOK_DIR)\n",
    "print(f\"PROJECT_ROOT gefunden in: {PROJECT_ROOT}\")\n",
    "\n",
    "# --- Params (Jetzt relativ zum PROJECT_ROOT) ---\n",
    "TARGET_PATH   = PROJECT_ROOT / \"data/processed/target.csv\"\n",
    "INDEX_COL     = \"date\"\n",
    "Y_COL         = \"IP_change\"\n",
    "OUTPUT_PATH   = PROJECT_ROOT / \"data/processed/tsfresh_slim.parquet\"\n",
    "\n",
    "WINDOW_SIZES  = [6, 12]\n",
    "N_JOBS        = 4\n",
    "\n",
    "# --- Imports ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_extraction.settings import EfficientFCParameters\n",
    "from tsfresh.utilities.dataframe_functions import roll_time_series\n",
    "\n",
    "# --- Load target & basic checks ---\n",
    "def load_y(path, y_col, index_col=\"date\"):\n",
    "    path = str(path)\n",
    "    if path.endswith(\".csv\"):\n",
    "        df = pd.read_csv(path, index_col=index_col, parse_dates=True)\n",
    "    elif path.endswith(\".parquet\"):\n",
    "        df = pd.read_parquet(path)\n",
    "        if index_col in df.columns:\n",
    "            df[index_col] = pd.to_datetime(df[index_col])\n",
    "            df = df.set_index(index_col)\n",
    "    else:\n",
    "        raise ValueError(\"Bitte .csv oder .parquet verwenden.\")\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"Target benötigt DatetimeIndex.\")\n",
    "    s = df[y_col] if (isinstance(df, pd.DataFrame) and y_col in df.columns) else df.iloc[:, 0]\n",
    "    s = pd.to_numeric(s, errors=\"coerce\")\n",
    "    return s\n",
    "\n",
    "y = load_y(TARGET_PATH, Y_COL, INDEX_COL).dropna()\n",
    "idx = y.index\n",
    "\n",
    "# --- Wide -> Long (eine Serie), Integer-Zeitachse ---\n",
    "long = (\n",
    "    y.to_frame(name=\"value\")\n",
    "     .reset_index()\n",
    "     .rename(columns={INDEX_COL: \"time\"})\n",
    ")\n",
    "long[\"time\"] = np.arange(len(long), dtype=np.int64)   # 0..n-1 (Integer-Zeit)\n",
    "long[\"id\"]   = f\"target.{Y_COL}\"\n",
    "long = long[[\"time\", \"id\", \"value\"]].sort_values([\"id\", \"time\"], kind=\"mergesort\")\n",
    "\n",
    "# --- Feature-Extraktion je Fenstergröße ---\n",
    "blocks = []\n",
    "for W in WINDOW_SIZES:\n",
    "    print(f\"[TSF] window={W}\")\n",
    "    # Rolling-Fenster: erzeugt Panel mit Fenstern der Länge W; Fensterende = jeweiliges t\n",
    "    rolled = roll_time_series(\n",
    "        long,\n",
    "        column_id=\"id\",\n",
    "        column_sort=\"time\",\n",
    "        min_timeshift=W - 1,     # erfordert volle W-Länge\n",
    "        max_timeshift=None       # alle möglichen Fenster\n",
    "    )\n",
    "\n",
    "    # Numerik absichern / lückenhafte Fenster verwerfen (kausal unkritisch)\n",
    "    rolled[\"value\"] = pd.to_numeric(rolled[\"value\"], errors=\"coerce\")\n",
    "    rolled = rolled.dropna(subset=[\"value\"])\n",
    "\n",
    "    # TSFresh-Parameter (performant & robust)\n",
    "    fc = EfficientFCParameters()\n",
    "\n",
    "    feats = extract_features(\n",
    "        rolled,\n",
    "        column_id=\"id\",\n",
    "        column_sort=\"time\",\n",
    "        default_fc_parameters=fc,\n",
    "        n_jobs=N_JOBS,\n",
    "        disable_progressbar=True,\n",
    "        pivot=True\n",
    "    )\n",
    "\n",
    "    # Fensterende (Integer-Zeit) pro rolled-id ermitteln und auf echten Zeitstempel mappen\n",
    "    end_map = rolled.groupby(\"id\", sort=False)[\"time\"].max()     # Series: rolled-id -> end_time (int)\n",
    "    feats[\"__end_time__\"] = feats.index.to_series().map(end_map)\n",
    "\n",
    "    # Nur gültige Fenster behalten und DatetimeIndex setzen (Index = t)\n",
    "    feats = feats.dropna(subset=[\"__end_time__\"])\n",
    "    feats[\"__time__\"] = feats[\"__end_time__\"].astype(int).map(\n",
    "        lambda t: idx[t] if (0 <= t < len(idx)) else pd.NaT\n",
    "    )\n",
    "    feats = (\n",
    "        feats.drop(columns=[\"__end_time__\"])\n",
    "             .dropna(subset=[\"__time__\"])\n",
    "             .set_index(\"__time__\")\n",
    "             .sort_index()\n",
    "    )\n",
    "\n",
    "    # Präfix & Typ; Spalten konsistent sortieren\n",
    "    feats = feats.add_prefix(f\"tsf_w{W}__\")\n",
    "    feats = feats.apply(pd.to_numeric, errors=\"coerce\").astype(\"float32\")\n",
    "    feats = feats.reindex(sorted(feats.columns), axis=1)\n",
    "\n",
    "    blocks.append(feats)\n",
    "\n",
    "# --- Zusammenführen & auf volle Zielachse reindexen ---\n",
    "if blocks:\n",
    "    OUT = pd.concat(blocks, axis=1).sort_index()\n",
    "else:\n",
    "    OUT = pd.DataFrame(index=idx)\n",
    "\n",
    "OUT = OUT.reindex(idx)                       # volle Zeitachse, frühe Monate ggf. NaN (erwartet)\n",
    "OUT = OUT.astype(\"float32\")\n",
    "\n",
    "# Optional: Spalten alphabetisch sortieren (stabile Ordnung)\n",
    "OUT = OUT.reindex(sorted(OUT.columns), axis=1)\n",
    "\n",
    "# --- Save ---\n",
    "Path(OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "OUT.to_parquet(OUTPUT_PATH)\n",
    "\n",
    "# --- Diagnostics ---\n",
    "wmax = max(WINDOW_SIZES) if WINDOW_SIZES else 0\n",
    "valid_slice = OUT.iloc[wmax:] if wmax > 0 else OUT\n",
    "nnr = float(valid_slice.notna().mean().mean()) if len(valid_slice) else float(\"nan\")\n",
    "print(f\"[TSF] wrote: {OUTPUT_PATH}  shape={OUT.shape}  non-null ratio after max window≈{nnr:.3f}\")\n",
    "\n",
    "# Hinweise:\n",
    "# - Kein zusätzlicher shift() beim späteren Merge: Index t ist bereits korrekt für Prognose von y_{t+1}.\n",
    "# - Standardisierung/Screening/DR in der Haupt-Pipeline strikt train-only fitten.\n",
    "# - Fehlende Werte am Anfang sind durch minimale Fensterlängen bedingt (z. B. W=12 -> erste 11 Zeilen NaN).\n"
   ],
   "id": "15b443b0a8336ba8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT gefunden in: /Users/jonasschernich/Documents/Masterarbeit/Code\n",
      "[TSF] window=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling: 100%|██████████| 20/20 [00:02<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSF] window=12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rolling: 100%|██████████| 20/20 [00:02<00:00,  9.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TSF] wrote: /Users/jonasschernich/Documents/Masterarbeit/Code/data/processed/tsfresh_slim_test.parquet  shape=(407, 1554)  non-null ratio after max window≈0.883\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
